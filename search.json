[{"path":"https://tripartio.github.io/ale/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://tripartio.github.io/ale/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"https://tripartio.github.io/ale/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"https://tripartio.github.io/ale/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"https://tripartio.github.io/ale/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"simulated-data-with-numeric-outcomes-aleplot-example-2","dir":"Articles","previous_headings":"","what":"Simulated data with numeric outcomes (ALEPlot Example 2)","title":"Comparison between `ALEPlot` and `ale` packages","text":"begin second code example directly ALEPlot package. (skip first example subset second, simply without interactions.) code example create simulated dataset train neural network : demonstration, x1 linear relationship y, x2 x3 non-linear relationships, x4 random variable relationship y. x1 x2 interact relationship y.","code":"## R code for Example 2 ## Load relevant packages library(ALEPlot) library(nnet)  ## Generate some data and fit a neural network supervised learning model set.seed(0)  # not in the original, but added for reproducibility n = 5000 x1 <- runif(n, min = 0, max = 1) x2 <- runif(n, min = 0, max = 1) x3 <- runif(n, min = 0, max = 1) x4 <- runif(n, min = 0, max = 1) y = 4*x1 + 3.87*x2^2 + 2.97*exp(-5+10*x3)/(1+exp(-5+10*x3))+ 13.86*(x1-0.5)*(x2-0.5)+ rnorm(n, 0, 1) DAT <- data.frame(y, x1, x2, x3, x4) nnet.DAT <- nnet(y~., data = DAT, linout = T, skip = F, size = 6, decay = 0.1, maxit = 1000, trace = F)"},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"aleplot-code","dir":"Articles","previous_headings":"Simulated data with numeric outcomes (ALEPlot Example 2)","what":"ALEPlot code","title":"Comparison between `ALEPlot` and `ale` packages","text":"create ALE data plots, ALEPlot requires creation custom prediction function: Now ALEPlot function can called create ALE data plot . function returns specially formatted list ALE data; can saved subsequent custom plotting.     ALEPlot implementation, calling function automatically prints plot. provides convenience user wants, convenient user want print plot point ALE creation. particularly inconvenient script building. Although possible configure R suspend graphic output ALEPlot called restart function call, straightforward—function give option control behaviour. ALE interactions can also calculated plotted:   output ALEPlot saved variables, contents can plotted finer user control using generic R plot method:","code":"## Define the predictive function yhat <- function(X.model, newdata) as.numeric(predict(X.model, newdata, type = \"raw\")) ## Calculate and plot the ALE main effects of x1, x2, x3, and x4 ALE.1 = ALEPlot(DAT[,2:5], nnet.DAT, pred.fun = yhat, J = 1, K = 500, NA.plot = TRUE) ALE.2 = ALEPlot(DAT[,2:5], nnet.DAT, pred.fun = yhat, J = 2, K = 500, NA.plot = TRUE) ALE.3 = ALEPlot(DAT[,2:5], nnet.DAT, pred.fun = yhat, J = 3, K = 500, NA.plot = TRUE) ALE.4 = ALEPlot(DAT[,2:5], nnet.DAT, pred.fun = yhat, J = 4, K = 500, NA.plot = TRUE) ## Calculate and plot the ALE second-order effects of {x1, x2} and {x1, x4} ALE.12 = ALEPlot(DAT[,2:5], nnet.DAT, pred.fun = yhat, J = c(1,2), K = 100, NA.plot = TRUE) ALE.14 = ALEPlot(DAT[,2:5], nnet.DAT, pred.fun = yhat, J = c(1,4), K = 100, NA.plot = TRUE) ## Manually plot the ALE main effects on the same scale for easier comparison ## of the relative importance of the four predictor variables par(mfrow = c(3,2)) plot(ALE.1$x.values, ALE.1$f.values, type=\"l\", xlab=\"x1\", ylab=\"ALE_main_x1\", xlim = c(0,1), ylim = c(-2,2), main = \"(a)\") plot(ALE.2$x.values, ALE.2$f.values, type=\"l\", xlab=\"x2\", ylab=\"ALE_main_x2\", xlim = c(0,1), ylim = c(-2,2), main = \"(b)\") plot(ALE.3$x.values, ALE.3$f.values, type=\"l\", xlab=\"x3\", ylab=\"ALE_main_x3\", xlim = c(0,1), ylim = c(-2,2), main = \"(c)\") plot(ALE.4$x.values, ALE.4$f.values, type=\"l\", xlab=\"x4\", ylab=\"ALE_main_x4\", xlim = c(0,1), ylim = c(-2,2), main = \"(d)\") ## Manually plot the ALE second-order effects of {x1, x2} and {x1, x4} image(ALE.12$x.values[[1]], ALE.12$x.values[[2]], ALE.12$f.values, xlab = \"x1\", ylab = \"x2\", main = \"(e)\") contour(ALE.12$x.values[[1]], ALE.12$x.values[[2]], ALE.12$f.values, add=TRUE, drawlabels=TRUE) image(ALE.14$x.values[[1]], ALE.14$x.values[[2]], ALE.14$f.values, xlab = \"x1\", ylab = \"x4\", main = \"(f)\") contour(ALE.14$x.values[[1]], ALE.14$x.values[[2]], ALE.14$f.values, add=TRUE, drawlabels=TRUE)"},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"ale-package-equivalent","dir":"Articles","previous_headings":"Simulated data with numeric outcomes (ALEPlot Example 2)","what":"ale package equivalent","title":"Comparison between `ALEPlot` and `ale` packages","text":"Now demonstrate functionality ale package. work model data, create . create model, invoke ale returns list various ALE elements. notable differences compared ALEPlot: tidyverse style, first element data second model. Unlike ALEPlot functions one variable time, ale generates ALE data multiple variables dataset . default, generates ALE elements predictor variables dataset given; user can specify single variable subset variables. cover details another vignette, purposes , note data element returns list ALE data variable plots element returns list ggplot plots. ale creates default generic predict function matches standard R models. prediction type default “response”, case, user can set desired type pred_type argument. However, complex non-standard prediction functions, ale supports custom functions pred_fun argument. Since plots saved list, can easily printed :  ale package plots various features enhance interpretability: outcome y displayed full original scale. median band shows middle 5 percentile y values displayed. idea ALE values outside band least somewhat significant. Similarly, 25% 75% percentile markers show middle 50% y values. ALE y value beyond bands indicates x variable strong alone values indicated can shift y value much. Rug plots indicate distribution data outliers -interpreted. might clear previous plots display exactly data shown ALEPlot. make comparison clearer, can recalculate ALEs zero-centred scale:  zero-centred plots, full range y values rug plots give context aids interpretation. (rugs look slightly different, randomly jittered avoid overplotting.) ale also produces interaction plots. Unlike ALEPlot, separate dedicated function, [ale_ixn()]. default, calculates possible two-way interactions variables data. variables interact , output data structure two-layer list, print code slightly complicated. However, sample code provide using functions purrr package iterating lists gridExtra package arranging plots reusable application.  interaction plots heat maps indicate interaction regions average value y colours. Grey indicates meaningful interaction; blue indicates positive interaction effect; red indicates negative effect. find easier interpret contour maps ALEPlot, especially since colours plot scale plots directly comparable . range outcome (y) values divided quantiles, deciles default. However, middle quantiles modified. Rather showing middle 10% 20% values, much narrow: shows middle 5%. (value based notion alpha 0.05 confidence intervals; can customized median_band argument.) legend shows midpoint y value quantile, usually mean boundaries quantile. exception special middle quantile, whose displayed midpoint value median entire dataset. interpretation interaction plots given region, interaction x1 x2 increases (blue) decreases (red) y amount indicated separate individual direct effects x1 x2 shown one-way ALE plots . indication total effect variables together rather additional effect interaction- beyond individual effects. Thus, x1-x2 interaction shows effect. interactions x3, even though x3 indeed strong effect y see one-way ALE plot , additional effect interaction variables, interaction plots entirely grey.","code":"library(ale) #> Loading required package: ggplot2  nn_ale <- ale(DAT, nnet.DAT, pred_type = \"raw\") #> Calculating ALE ■                                  0% |  ETA: ? # , fig.asp=3 is OK # Print plots gridExtra::grid.arrange(grobs = nn_ale$plots, ncol = 1) # Zero-centred ALE nn_ale <- ale(DAT, nnet.DAT, pred_type = \"raw\", relative_y = 'zero')  gridExtra::grid.arrange(grobs = nn_ale$plots, ncol = 2) # Create and plot interactions nn_ale_ixn <- ale_ixn(DAT, nnet.DAT, pred_type = \"raw\") #> Calculating ALE interactions ■■■■■■■■■                         25% |  ETA:  1s  # Print plots nn_ale_ixn$plots |>   purrr::walk(\\(.x1) {  # extract list of x1 ALE outputs     gridExtra::grid.arrange(grobs = .x1, ncol = 1)  # plot all x1 plots   })"},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"real-data-with-binary-outcomes-aleplot-example-3","dir":"Articles","previous_headings":"","what":"Real data with binary outcomes (ALEPlot Example 3)","title":"Comparison between `ALEPlot` and `ale` packages","text":"next code example ALEPlot package analyzes real dataset binary outcome variable. Whereas ALEPlot user load CSV file might readily available, make dataset available census dataset. load adjustments necessary run ALEPlot example. Although gradient boosted trees generally perform quite well, rather slow. Rather wait run, code downloads pretrained GBM model. However, code used generate provided comments can see run want . Note model calls based data[,-c(3,4)], drops third fourth variables (fnlwgt education, respectively).","code":"## R code for Example 3 ## Load relevant packages library(ALEPlot) library(gbm) #> Loaded gbm 2.1.8.1  ## Read data and fit a boosted tree supervised learning model data(census, package = 'ale')  # load ale package version of the data data <-     census |>    as.data.frame() |>   # ALEPlot is not compatible with the tibble format   select(age:native_country, higher_income) |>  # Rearrange columns to match ALEPlot order   na.omit(data) # To generate the code, uncomment the following lines. # But it is slow, so this vignette loads a pre-created model object. # set.seed(0) # gbm.data <- gbm(higher_income ~ ., data= data[,-c(3,4)], #                 distribution = \"bernoulli\", n.trees=6000, shrinkage=0.02, #                 interaction.depth=3) # saveRDS(gbm.data, file.choose()) gbm.data <- url('https://github.com/Tripartio/ale/raw/main/download/gbm.data_model.rds') |>    readRDS()  gbm.data #> gbm(formula = higher_income ~ ., distribution = \"bernoulli\",  #>     data = data[, -c(3, 4)], n.trees = 6000, interaction.depth = 3,  #>     shrinkage = 0.02) #> A gradient boosted model with bernoulli loss function. #> 6000 iterations were performed. #> There were 12 predictors of which 12 had non-zero influence."},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"aleplot-code-1","dir":"Articles","previous_headings":"Real data with binary outcomes (ALEPlot Example 3)","what":"ALEPlot code","title":"Comparison between `ALEPlot` and `ale` packages","text":", create custom prediction function call ALEPlot function generate plots. prediction type “link”, represents log odds gbm package. Creation ALE plots rather slow gbm predict function slow. example, age, education_num (number years education), hours_per_week plotted, along interaction age hours_per_week.","code":"## Define the predictive function; note the additional arguments for the ## predict function in gbm yhat <- function(X.model, newdata) as.numeric(predict(X.model, newdata, n.trees = 6000, type=\"link\"))  ## Calculate and plot the ALE main and interaction effects for x_1, x_3, ## x_11, and {x_1, x_11} par(mfrow = c(2,2), mar = c(4,4,2,2)+ 0.1) ALE.1=ALEPlot(data[,-c(3,4,15)], gbm.data, pred.fun=yhat, J=1, K=500, NA.plot = TRUE) ALE.3=ALEPlot(data[,-c(3,4,15)], gbm.data, pred.fun=yhat, J=3, K=500, NA.plot = TRUE) ALE.11=ALEPlot(data[,-c(3,4,15)], gbm.data, pred.fun=yhat, J=11, K=500, NA.plot = TRUE) ALE.1and11=ALEPlot(data[,-c(3,4,15)], gbm.data, pred.fun=yhat, J=c(1,11), K=50, NA.plot = FALSE)"},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"ale-package-equivalent-1","dir":"Articles","previous_headings":"Real data with binary outcomes (ALEPlot Example 3)","what":"ale package equivalent","title":"Comparison between `ALEPlot` and `ale` packages","text":"analogous code using ale package. case, also need define custom predict function particular n.trees = 6000 argument. speed things , provide pretrained ale object. possible ale returns objects data plots bundled together side effects (like automatic printing created plots). (probably possible similarly cache ALEPlot ALE objects, quite straightforward.)","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"log-odds","dir":"Articles","previous_headings":"Real data with binary outcomes (ALEPlot Example 3) > ale package equivalent","what":"Log odds","title":"Comparison between `ALEPlot` and `ale` packages","text":"display plots easy ale package focus age, education_num, hours_per_week comparison ALEPlot. shapes plots look different, ale tries much possible display plots y-axis coordinate scale easy comparison across plots.  Now generate ALE data two-way interactions plot . , note interaction age hours_per_week. interaction minimal except extremely high cases hours per week.","code":"# Custom predict function that returns log odds yhat <- function(object, newdata) {   as.numeric(     predict(object, newdata,  n.trees = 6000,             type=\"link\")  # return log odds   ) }  # Generate ALE data for all variables  # # To generate the code, uncomment the following lines. # # But it is slow, so this vignette loads a pre-created model object. # gbm_ale_link <- ale( #   data[,-c(3,4)], gbm.data, #   pred_fun = yhat, #   x_intervals = 500, #   rug_sample_size = 600,  # technical issue: rug_sample_size must be > x_intervals + 1 #   relative_y = 'zero'  # compatibility with ALEPlot # ) # saveRDS(gbm_ale_link, file.choose()) gbm_ale_link <- url('https://github.com/Tripartio/ale/raw/main/download/gbm_ale_link.rds') |>    readRDS()  # Print plots gridExtra::grid.arrange(grobs = gbm_ale_link$plots, ncol = 2) # # To generate the code, uncomment the following lines. # # But it is slow, so this vignette loads a pre-created model object. # gbm_ale_ixn_link <- ale_ixn( #   data[,-c(3,4)], gbm.data, #   pred_fun = yhat, #   x_intervals = 500, #   rug_sample_size = 600,  # technical issue: rug_sample_size must be > x_intervals + 1 #   relative_y = 'zero'  # compatibility with ALEPlot # ) # saveRDS(gbm_ale_ixn_link, file.choose()) gbm_ale_ixn_link <- url('https://github.com/Tripartio/ale/raw/main/download/gbm_ale_ixn_link.rds') |>    readRDS()  # Print plots gbm_ale_ixn_link$plots |>   purrr::walk(\\(.x1) {  # extract list of x1 ALE outputs     gridExtra::grid.arrange(grobs = .x1, ncol = 2)  # plot all x1 interaction plots   })"},{"path":"https://tripartio.github.io/ale/articles/ale-ALEPlot.html","id":"predicted-probabilities","dir":"Articles","previous_headings":"Real data with binary outcomes (ALEPlot Example 3) > ale package equivalent","what":"Predicted probabilities","title":"Comparison between `ALEPlot` and `ale` packages","text":"Log odds necessarily interpretable way express probabilities (though show shortly sometimes uniquely valuable). , repeat ALE creation using “response” prediction type probabilities default median centring plots. can see, shapes plots similar, y axes easily interpretable probability (0 1) census respondent higher income category. median around 10% indicates median prediction GBM model: half respondents predicted higher 10% likelihood higher income half predicted lower likelihood. y-axis rug plots indicate predictions generally rather extreme, either relatively close 0 1, predictions middle.  Finally, generate two-way interactions, time based probabilities instead log odds. However, probabilities might best choice indicating interactions , see rugs one-way ALE plots, GBM model heavily concentrates probabilities extremes near 0 1. Thus, plots’ suggestions strong interactions likely exaggerated. case, log odds ALEs shown probably relevant.","code":"# Custom predict function that returns predicted probabilities yhat <- function(object, newdata) {   as.numeric(     predict(object, newdata,  n.trees = 6000,             type=\"response\")  # return predicted probabilities   ) }  # Generate ALE data for all variables  # # To generate the code, uncomment the following lines. # # But it is slow, so this vignette loads a pre-created model object. # gbm_ale_prob <- ale( #   data[,-c(3,4)], gbm.data, #   pred_fun = yhat, #   x_intervals = 500, #   rug_sample_size = 600  # technical issue: rug_sample_size must be > x_intervals + 1 # ) # saveRDS(gbm_ale_prob, file.choose()) gbm_ale_prob <- url('https://github.com/Tripartio/ale/raw/main/download/gbm_ale_prob.rds') |>    readRDS()  # Print plots gridExtra::grid.arrange(grobs = gbm_ale_prob$plots, ncol = 2) # # To generate the code, uncomment the following lines. # # But it is slow, so this vignette loads a pre-created model object. # gbm_ale_ixn_prob <- ale_ixn( #   data[,-c(3,4)], gbm.data, #   pred_fun = yhat, #   x_intervals = 500, #   rug_sample_size = 600  # technical issue: rug_sample_size must be > x_intervals + 1 # ) # saveRDS(gbm_ale_ixn_prob, file.choose()) gbm_ale_ixn_prob <- url('https://github.com/Tripartio/ale/raw/main/download/gbm_ale_ixn_prob.rds') |>    readRDS()  # Print plots gbm_ale_ixn_prob$plots |>   purrr::walk(\\(.x1) {  # extract list of x1 ALE outputs     gridExtra::grid.arrange(grobs = .x1, ncol = 2)  # plot all x1 plots   })"},{"path":"https://tripartio.github.io/ale/articles/ale-intro.html","id":"diamonds-dataset","dir":"Articles","previous_headings":"","what":"diamonds dataset","title":"Introduction to the `ale` package","text":"introduction, use diamonds dataset, built-ggplot2 graphics system. cleaned original version removing duplicates invalid entries length (x), width (y), depth (z) 0. description modified dataset. machine learning analysis intends extrapolate results future data, must first split dataset training test samples. model developed training set evaluated test set. (dataset small feasibly split training test sets, ale package tools appropriately handle small datasets. , now split dataset 80-20 split training set 31,841 rows test set 7,898 rows. Now can build model.","code":"# Clean up some invalid entries diamonds <- ggplot2::diamonds |>    filter(!(x == 0 | y == 0 | z == 0)) |>    # https://lorentzen.ch/index.php/2021/04/16/a-curious-fact-on-the-diamonds-dataset/   distinct(     price, carat, cut, color, clarity,     .keep_all = TRUE   ) |>    rename(     x_length = x,     y_width = y,     z_depth = z,     depth_pct = depth   )  summary(diamonds) #>      carat               cut        color       clarity       depth_pct     #>  Min.   :0.2000   Fair     : 1492   D:4658   SI1    :9857   Min.   :43.00   #>  1st Qu.:0.5200   Good     : 4173   E:6684   VS2    :8227   1st Qu.:61.00   #>  Median :0.8500   Very Good: 9714   F:6998   SI2    :7916   Median :61.80   #>  Mean   :0.9033   Premium  : 9657   G:7815   VS1    :6007   Mean   :61.74   #>  3rd Qu.:1.1500   Ideal    :14703   H:6443   VVS2   :3463   3rd Qu.:62.60   #>  Max.   :5.0100                     I:4556   VVS1   :2413   Max.   :79.00   #>                                     J:2585   (Other):1856                   #>      table           price          x_length         y_width       #>  Min.   :43.00   Min.   :  326   Min.   : 3.730   Min.   : 3.680   #>  1st Qu.:56.00   1st Qu.: 1410   1st Qu.: 5.160   1st Qu.: 5.170   #>  Median :57.00   Median : 3365   Median : 6.040   Median : 6.040   #>  Mean   :57.58   Mean   : 4686   Mean   : 6.009   Mean   : 6.012   #>  3rd Qu.:59.00   3rd Qu.: 6406   3rd Qu.: 6.730   3rd Qu.: 6.720   #>  Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900   #>                                                                    #>     z_depth       #>  Min.   : 1.070   #>  1st Qu.: 3.190   #>  Median : 3.740   #>  Mean   : 3.711   #>  3rd Qu.: 4.150   #>  Max.   :31.800   #> str(diamonds) #> tibble [39,739 × 10] (S3: tbl_df/tbl/data.frame) #>  $ carat    : num [1:39739] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... #>  $ cut      : Ord.factor w/ 5 levels \"Fair\"<\"Good\"<..: 5 4 2 4 2 3 3 3 1 3 ... #>  $ color    : Ord.factor w/ 7 levels \"D\"<\"E\"<\"F\"<\"G\"<..: 2 2 2 6 7 7 6 5 2 5 ... #>  $ clarity  : Ord.factor w/ 8 levels \"I1\"<\"SI2\"<\"SI1\"<..: 2 3 5 4 2 6 7 3 4 5 ... #>  $ depth_pct: num [1:39739] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... #>  $ table    : num [1:39739] 55 61 65 58 58 57 57 55 61 61 ... #>  $ price    : int [1:39739] 326 326 327 334 335 336 336 337 337 338 ... #>  $ x_length : num [1:39739] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... #>  $ y_width  : num [1:39739] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... #>  $ z_depth  : num [1:39739] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... summary(diamonds$price) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>     326    1410    3365    4686    6406   18823 # Split the dataset into training and test sets # https://stackoverflow.com/a/54892459/2449926 set.seed(0) train_test_split <- sample(c(TRUE, FALSE), nrow(diamonds), replace = TRUE, prob = c(0.8, 0.2)) diamonds_train <- diamonds[train_test_split, ] diamonds_test <- diamonds[!train_test_split, ]"},{"path":"https://tripartio.github.io/ale/articles/ale-intro.html","id":"modelling-with-general-additive-models-gam","dir":"Articles","previous_headings":"","what":"Modelling with general additive models (GAM)","title":"Introduction to the `ale` package","text":"ALE model-agnostic IML approach, , works kind machine learning model. , ale works R model condition can predict numeric outcomes (raw estimates regression probabilities odds ratios classification). demonstration, use general additive models (GAM), relatively fast algorithm models data flexibly ordinary least squares regression. beyond scope explain GAM works (can learn Noam Ross’s excellent tutorial), examples work machine learning algorithm. train GAM model predict diamond prices:","code":"# Create a GAM model with flexible curves to predict diamond prices. # (In testing, mgcv::gam actually performed better than nnet.) # Smooth all numeric variables and include all other variables # Build model on training data, not on the full dataset. gam_diamonds <- mgcv::gam(   price ~ s(carat) + s(depth_pct) + s(table) + s(x_length) + s(y_width) + s(z_depth) +     cut + color + clarity,   data = diamonds_train   ) summary(gam_diamonds) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> price ~ s(carat) + s(depth_pct) + s(table) + s(x_length) + s(y_width) +  #>     s(z_depth) + cut + color + clarity #>  #> Parametric coefficients: #>               Estimate Std. Error  t value Pr(>|t|)     #> (Intercept)  4413.8270    14.6887  300.490  < 2e-16 *** #> cut.L         282.9005    42.6670    6.630 3.40e-11 *** #> cut.Q           0.5949    29.9527    0.020 0.984153     #> cut.C          59.0119    22.1652    2.662 0.007763 **  #> cut^4          38.5756    15.9970    2.411 0.015896 *   #> color.L     -2154.1921    21.2064 -101.582  < 2e-16 *** #> color.Q      -698.0122    19.4190  -35.945  < 2e-16 *** #> color.C       -66.5582    18.2415   -3.649 0.000264 *** #> color^4        75.6030    17.0393    4.437 9.15e-06 *** #> color^5      -116.1725    16.1289   -7.203 6.03e-13 *** #> color^6       -46.9826    15.0328   -3.125 0.001778 **  #> clarity.L    4174.5052    37.6891  110.762  < 2e-16 *** #> clarity.Q   -1571.3435    35.1359  -44.722  < 2e-16 *** #> clarity.C     807.3063    30.3669   26.585  < 2e-16 *** #> clarity^4    -253.7668    24.6400  -10.299  < 2e-16 *** #> clarity^5     211.8216    20.4734   10.346  < 2e-16 *** #> clarity^6      37.3081    18.0393    2.068 0.038633 *   #> clarity^7     129.6810    15.9079    8.152 3.71e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Approximate significance of smooth terms: #>                edf Ref.df       F p-value     #> s(carat)     8.083  8.748  21.871  <2e-16 *** #> s(depth_pct) 6.924  7.869   7.081  <2e-16 *** #> s(table)     4.035  5.061   2.620  0.0199 *   #> s(x_length)  7.284  8.023  39.995  <2e-16 *** #> s(y_width)   8.679  8.878 153.594  <2e-16 *** #> s(z_depth)   8.675  8.868  10.990  <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> R-sq.(adj) =  0.929   Deviance explained = 92.9% #> GCV = 1.2564e+06  Scale est. = 1.254e+06  n = 31841"},{"path":"https://tripartio.github.io/ale/articles/ale-intro.html","id":"ale-function-for-generating-ale-data-and-plots","dir":"Articles","previous_headings":"","what":"ale function for generating ALE data and plots","title":"Introduction to the `ale` package","text":"core function ale package ale function. Consistent tidyverse conventions, first argument dataset. second argument model object–R model object can generate numeric predictions acceptable. default, generates ALE data plots input variables used model. change options (e.g., calculate ALE subset variables; output data plots rather ; use custom, non-standard predict function model), see details help file function: help(ale). ale function returns list various elements. two main ones data, containing ALE x intervals y values interval, plots, containing ALE plots individual ggplot objects. elements list one element per input variable. function also returns several details outcome (y) variable important parameters used ALE calculation. Another important element stats, containing ALE-based statistics, describe separate vignette. access plot specific variable, can call variable name element plots element. ggplot objects, easy manipulate. example, access print carat ALE plot, simply call ale_gam_diamonds$plots$carat :  iterate list plot ALE plots, provide demonstration code using gridExtra package arranging multiple plots common plot grid using gridExtra::grid.arrange. need pass list plots grobs argument can specify want two plots per row ncol argument.","code":"# Simple ALE without bootstrapping ale_gam_diamonds <- ale(diamonds_test, gam_diamonds) #> Calculating ALE ■                                  0% |  ETA: ? #> Calculating ALE ■■■■■■■■■■■■■■■■■■                56% |  ETA:  2s # Print a plot by entering its reference ale_gam_diamonds$plots$carat # Print all plots gridExtra::grid.arrange(grobs = ale_gam_diamonds$plots, ncol = 2)"},{"path":"https://tripartio.github.io/ale/articles/ale-intro.html","id":"bootstrapped-ale","dir":"Articles","previous_headings":"","what":"Bootstrapped ALE","title":"Introduction to the `ale` package","text":"One key features ALE package bootstrapping ALE results ensure results reliable, , generalizable data beyond sample model built. external generalization requires analyzing ale function test data distinct training data. samples small , provide different bootstrapping method, [model_bootstrap()], explained vignette small datasets. Although ALE faster IML techniques global explanation partial dependence plots (PDP) SHAP, still requires time run. Bootstrapping multiplies time number bootstrap iterations. Since vignette just demonstration package functionality rather real analysis, demonstrate bootstrapping small subset test data. run much faster speed ALE algorithm depends size dataset. , let us take random sample 200 rows test set. Now create bootstrapped ALE data plots using boot_it argument. ALE relatively stable IML algorithm (compared others like PDP), 100 bootstrap samples sufficient relatively stable results, especially model development. Final results confirmed 1000 bootstrap samples , much difference results beyond 100 iterations. However, introduction runs faster, demonstrate 10 iterations.  case, bootstrapped results mostly similar single (non-bootstrapped) ALE result. principle, always bootstrap results trust bootstrapped results. unusual result values x_length (length diamond) 6.2 mm higher associated lower diamond prices. compare y_width value (width diamond), suspect length width (, size) diamond become increasingly large, price increases much rapidly width length width inordinately high effect tempered decreased effect length high values. worth exploration real analysis, just introducing key features package.","code":"# Bootstraping is rather slow, so create a smaller subset of new data for demonstration set.seed(0) new_rows <- sample(nrow(diamonds_test), 200, replace = FALSE) diamonds_small_test <- diamonds_test[new_rows, ] # Normally boot_it should be set to 100, but just 10 here for a faster demonstration ale_gam_diamonds_boot <- ale(diamonds_small_test, gam_diamonds, boot_it = 10) #> Calculating ALE ■                                  0% |  ETA: ? #> Calculating ALE ■■■■■■■■■■■■■■                    44% |  ETA:  2s  # Bootstrapping produces confidence intervals gridExtra::grid.arrange(grobs = ale_gam_diamonds_boot$plots, ncol = 2)"},{"path":"https://tripartio.github.io/ale/articles/ale-intro.html","id":"ale-interactions","dir":"Articles","previous_headings":"","what":"ALE interactions","title":"Introduction to the `ale` package","text":"Another advantage ALE provides data two-way interactions variables. implemented ale::ale_ixn function. Like ale function, [ale_ixn()] similarly requires input dataset model object. default, generates ALE data plots possible pairs input variables used model. However, ALE interaction requires least one variables numeric. , [ale_ixn()] notion x1 x2 variables; x1 variable must numeric whereas x2 can input datatype. change default options (e.g., calculate interactions certain pairs variables), see details help file function: help(ale_ixn). Like ale function, [ale_ixn()] returns list one element per input x1 variable, well .common_data element details outcome (y) variable. However, case, variable’s element consists list x2 variables x1 interaction calculated. x2 element two elements: ALE data variable ggplot plot object plots ALE data. interaction plots, x1 variable always shown x axis x2 variable y axis. , provide demonstration code plot ALE plots. little complex time two levels interacting variables output data, use purrr package iterate list structure. purrr::walk takes list first argument specify anonymous function want element list. specify anonymous function \\(.x1) {...} .x1 case represents individual element ale_ixn_gam_diamonds$plots turn, , sublist plots x1 variable interacts. print plots x1 interactions combined grid plots gridExtra::grid.arrange, .  printing plots together gridExtra::grid.arrange statement, might appear vertically distorted plot forced height. fine-tuned presentation, need refer specific plot. example, can print interaction plot carat depth referring thus: ale_ixn_gam_diamonds$plots$carat$depth.  best dataset use illustrate ALE interactions none . expressed graphs ALE y values falling middle grey band (median band), indicates interactions shift price outside middle 5% values. words, meaningful interaction effect. Note ALE interactions particular: ALE interaction means two variables composite effect separate independent effects. , course x_length y_width effects price, one-way ALE plots show, additional composite effect. see ALE interaction plots look like presence interactions, see ALEPlot comparison vignette, explains interaction plots detail.","code":"# ALE two-way interactions ale_ixn_gam_diamonds <- ale_ixn(diamonds_test, gam_diamonds) #> Calculating ALE interactions ■                                  0% |  ETA: ? #> Calculating ALE interactions ■■■■■■                            17% |  ETA: 22s #> Calculating ALE interactions ■■■■■■■■■■■                       33% |  ETA: 15s #> Calculating ALE interactions ■■■■■■■■■■■■■■■■                  50% |  ETA: 10s #> Calculating ALE interactions ■■■■■■■■■■■■■■■■■■■■■             67% |  ETA:  6s # Print all interaction plots ale_ixn_gam_diamonds$plots |>   purrr::walk(\\(.x1) {  # extract list of x1 ALE outputs     gridExtra::grid.arrange(grobs = .x1, ncol = 2)  # plot each x1 plot   }) ale_ixn_gam_diamonds$plots$carat$depth"},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"what-is-a-small-dataset","dir":"Articles","previous_headings":"","what":"What is a “small” dataset?","title":"Analyzing small datasets (<2000 rows) with ALE","text":"obvious question , “small ‘small’?” complex question way beyond scope vignette try answer rigorously. can simply say key issue stake applying training-test split common machine learning crucial technique increasing generalizability data analysis. , question becomes focused , “small small training-test split machine learning analysis?” rule thumb familiar machine learning requires least 200 rows data predictor variable. , example, five input variables, need least 1000 rows data. note refer size entire dataset minimum size training subset. , carry 80-20 split full dataset (, 80% training set), need least 1000 rows training set another 250 rows test set, minimum 1250 rows. (carry hyperparameter tuning cross validation training set, need even data.) see headed, might quickly realize datasets less 2000 rows probably “small”. can see even many datasets 2000 rows nonetheless “small”, probably need techniques mentioned vignette.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"attitude-dataset","dir":"Articles","previous_headings":"","what":"attitude dataset","title":"Analyzing small datasets (<2000 rows) with ALE","text":"analyses use attitude dataset, built-R: “survey clerical employees large financial organization, data aggregated questionnaires approximately 35 employees 30 (randomly selected) departments.” Since ’re talking “small” datasets, figure might well demonstrate principles extremely small examples.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"description","dir":"Articles","previous_headings":"attitude dataset","what":"Description","title":"Analyzing small datasets (<2000 rows) with ALE","text":"survey clerical employees large financial organization, data aggregated questionnaires approximately 35 employees 30 (randomly selected) departments. numbers give percent proportion favourable responses seven questions department.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"format","dir":"Articles","previous_headings":"attitude dataset","what":"Format","title":"Analyzing small datasets (<2000 rows) with ALE","text":"data frame 30 observations 7 variables. first column short names reference, second one variable names data frame:","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"source","dir":"Articles","previous_headings":"attitude dataset","what":"Source","title":"Analyzing small datasets (<2000 rows) with ALE","text":"Chatterjee, S. Price, B. (1977) Regression Analysis Example. New York: Wiley. (Section 3.7, p.68ff 2nd ed.(1991).) first run ALE analysis dataset valid regular dataset, even though small proper training-test split. small-scale demonstration mainly demonstrate ale package valid analyzing even small datasets, just large datasets typically used machine learning.","code":"str(attitude) #> 'data.frame':    30 obs. of  7 variables: #>  $ rating    : num  43 63 71 61 81 43 58 71 72 67 ... #>  $ complaints: num  51 64 70 63 78 55 67 75 82 61 ... #>  $ privileges: num  30 51 68 45 56 49 42 50 72 45 ... #>  $ learning  : num  39 54 69 47 66 44 56 55 67 47 ... #>  $ raises    : num  61 63 76 54 71 54 66 70 71 62 ... #>  $ critical  : num  92 73 86 84 83 49 68 66 83 80 ... #>  $ advance   : num  45 47 48 35 47 34 35 41 31 41 ... summary(attitude) #>      rating        complaints     privileges       learning         raises      #>  Min.   :40.00   Min.   :37.0   Min.   :30.00   Min.   :34.00   Min.   :43.00   #>  1st Qu.:58.75   1st Qu.:58.5   1st Qu.:45.00   1st Qu.:47.00   1st Qu.:58.25   #>  Median :65.50   Median :65.0   Median :51.50   Median :56.50   Median :63.50   #>  Mean   :64.63   Mean   :66.6   Mean   :53.13   Mean   :56.37   Mean   :64.63   #>  3rd Qu.:71.75   3rd Qu.:77.0   3rd Qu.:62.50   3rd Qu.:66.75   3rd Qu.:71.00   #>  Max.   :85.00   Max.   :90.0   Max.   :83.00   Max.   :75.00   Max.   :88.00   #>     critical        advance      #>  Min.   :49.00   Min.   :25.00   #>  1st Qu.:69.25   1st Qu.:35.00   #>  Median :77.50   Median :41.00   #>  Mean   :74.77   Mean   :42.93   #>  3rd Qu.:80.00   3rd Qu.:47.75   #>  Max.   :92.00   Max.   :72.00"},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"ale-for-ordinary-least-squares-regression-multiple-linear-regression","dir":"Articles","previous_headings":"","what":"ALE for ordinary least squares regression (multiple linear regression)","title":"Analyzing small datasets (<2000 rows) with ALE","text":"Ordinary least squares (OLS) regression generic multivariate statistical technique. Thus, use baseline illustration help motivate value ALE interpreting analysis small data samples. train OLS model predict average rating: least, ale useful visualizing effects model variables. Note now, run ale bootstrapping (default) small samples require special bootstrap approach, explained . now, using ALE accurately visualize model estimates.  visualization confirms see model coefficients : complaints strong positive effect ratings learning moderate effect. However, ALE indicates stronger effect advance regression coefficients suggest. variables relatively little effect ratings. see shortly proper bootstrapping model can shed light discrepancies. unique ALE compared approaches visualizes effect variable irrespective interactions might might exist variables, whether interacting variables included model . can use [ale_ixn()] visualize possible existence interactions:  powerful use-case ale package: can used explore existence interactions fact; need hypothesized beforehand. However, without bootstrapping, findings considered reliable. case, interactions dataset, continue exploring .","code":"lm_attitude <- lm(rating ~ ., data = attitude)  summary(lm_attitude) #>  #> Call: #> lm(formula = rating ~ ., data = attitude) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -10.9418  -4.3555   0.3158   5.5425  11.5990  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept) 10.78708   11.58926   0.931 0.361634     #> complaints   0.61319    0.16098   3.809 0.000903 *** #> privileges  -0.07305    0.13572  -0.538 0.595594     #> learning     0.32033    0.16852   1.901 0.069925 .   #> raises       0.08173    0.22148   0.369 0.715480     #> critical     0.03838    0.14700   0.261 0.796334     #> advance     -0.21706    0.17821  -1.218 0.235577     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 7.068 on 23 degrees of freedom #> Multiple R-squared:  0.7326, Adjusted R-squared:  0.6628  #> F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05 ale_lm_attitude_simple <- ale(attitude, lm_attitude) #> Calculating ALE ■                                  0% |  ETA: ?  # Print all plots gridExtra::grid.arrange(grobs = ale_lm_attitude_simple$plots, ncol = 2) ale_lm_attitude_ixn <- ale_ixn(attitude, lm_attitude) #> Calculating ALE interactions ■■■■■■                            17% |  ETA:  1s  # Print plots ale_lm_attitude_ixn$plots |>   purrr::walk(\\(.x1) {  # extract list of x1 ALE outputs     gridExtra::grid.arrange(grobs = .x1, ncol = 2)  # plot all x1 plots   })"},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"full-model-bootstrapping","dir":"Articles","previous_headings":"","what":"Full model bootstrapping","title":"Analyzing small datasets (<2000 rows) with ALE","text":"referred frequently importance bootstrapping. None model results, without ALE, considered reliable without bootstrapped. large datasets clear separation training testing samples, ale bootstraps ALE results test data. However, dataset small subdivided training test sets, entire model bootstrapped. , multiple models trained, one bootstrap sample. reliable results average results bootstrap models, however many . [model_bootstrap()] function automatically carries full-model bootstrapping suitable small datasets. Specifically, : Creates multiple bootstrap samples (default 100; user can specify number); Creates model bootstrap sample; Calculates model overall statistics, variable coefficients, ALE values model bootstrap sample; Calculates mean, median, lower upper confidence intervals values across bootstrap samples. [model_bootstrap()] two required arguments. Consistent tidyverse conventions, first argument dataset, data. second argument model object analyzed. objects follow standard R modelling conventions, [model_bootstrap()] able automatically recognize parse model object. , call [model_bootstrap()]: default, [model_bootstrap()] creates 100 bootstrap samples provided dataset creates 100 + 1 models data (one bootstrap sample original dataset). (However, illustration runs faster, demonstrate 10 iterations.) Beyond ALE data, also provides bootstrapped overall model statistics (provided [broom::glance()]) bootstrapped model coefficients (provided [broom::tidy()]). default options [broom::glance()], [broom::tidy()], [ale()] can customized, along defaults [model_bootstrap()], number bootstrap iterations. can consult help file details help(model_bootstrap). [model_bootstrap()] returns list following elements (depending values requested output argument: model_stats: bootstrapped results [broom::glance()] model_coefs: bootstrapped results [broom::tidy()] ale_data: bootstrapped ALE data plots boot_data: full bootstrap data (returned default) bootstrapped overall model statistics: bootstrapped model coefficients: can visualize results ALE plots.  key interpreting effects models contrasting grey bootstrapped confidence bands surrounding average (median) ALE effect thin horizontal grey band labelled ‘median \\(\\pm\\) 2.5%’. Anything within \\(\\pm\\) 2.5% median 5% middle data. bootstrapped effects clearly beyond middle band may considered significant. criteria, considering median rating 65.5%, can conclude : Complaints handled around 68% led -average overall ratings; complaints handled around 72% associated -average overall ratings. 95% bootstrapped confidence intervals every variable fully overlap entire 5% median band. Thus, despite general trends data (particular learning’s positive trend advance’s negative trend), data support claims factor convincingly meaningful effect ratings. Although basic demonstration, readily shows crucial proper bootstrapping make meaningful inferences data analysis.","code":"mb_lm <- model_bootstrap(   attitude,    lm_attitude,   boot_it = 10,  # 100 by default but reduced here for a faster demonstration   silent = TRUE  # progress bars disabled for the vignette ) mb_lm$model_stats #> # A tibble: 8 × 7 #>   name            estimate conf.low       mean       median  conf.high        sd #>   <chr>              <dbl>    <dbl>      <dbl>        <dbl>      <dbl>     <dbl> #> 1 r.squared      0.793      6.78e-1  0.793      0.822        0.874     0.0758    #> 2 adj.r.squared  0.739      5.94e-1  0.739      0.775        0.841     0.0956    #> 3 sigma          6.03       4.62e+0  6.03       5.91         7.65      1.05      #> 4 statistic     16.9        8.07e+0 16.9       17.7         26.7       6.86      #> 5 p.value        0.0000203  3.53e-9  0.0000203  0.000000159  0.0000922 0.0000362 #> 6 df             6          6   e+0  6          6            6         0         #> 7 df.residual   23          2.3 e+1 23         23           23         0         #> 8 nobs          30          3   e+1 30         30           30         0 mb_lm$model_coefs #> # A tibble: 7 × 7 #>   term        estimate conf.low    mean  median conf.high std.error #>   <chr>          <dbl>    <dbl>   <dbl>   <dbl>     <dbl>     <dbl> #> 1 (Intercept)   8.40   -15.4     8.40    6.57      37.1      19.9   #> 2 complaints    0.561    0.370   0.561   0.556      0.772     0.144 #> 3 privileges   -0.0575  -0.325  -0.0575  0.0323     0.187     0.199 #> 4 learning      0.227    0.0385  0.227   0.233      0.434     0.131 #> 5 raises        0.215   -0.0105  0.215   0.169      0.472     0.179 #> 6 critical      0.0300  -0.303   0.0300  0.120      0.302     0.235 #> 7 advance      -0.173   -0.509  -0.173  -0.0816     0.133     0.239 gridExtra::grid.arrange(grobs = mb_lm$ale$plots, ncol = 2)"},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"ale-for-general-additive-models-gam","dir":"Articles","previous_headings":"","what":"ALE for general additive models (GAM)","title":"Analyzing small datasets (<2000 rows) with ALE","text":"major limitation OLS regression models relationships x variables y straight lines. unlikely relationships truly linear. OLS accurately capture non-linear relationships. samples relatively small, use general additive models (GAM) modelling. grossly oversimplify things, GAM extension statistical regression analysis lets model fit flexible patterns data instead restricted best-fitting straight line. ideal approach samples small machine learning provides flexible curves unlike ordinary least squares regression yet overfit excessively machine learning techniques working small samples. GAM, variables want become flexible need wrapped s (smooth) function, e.g., s(complaints). example, smooth numerical input variables: comparing adjusted R2 OLS model (0.663) GAM model (0.776), can readily see GAM model provides superior fit data. understand variables responsible relationship, results smooth terms GAM readily interpretable. need visualized effective interpretation—ALE perfect purposes.  Compared OLS results , GAM results provide quite surprise concerning shape effect employees’ perceptions department critical–seems low criticism high criticism negatively affect ratings. However, trying interpret results, must remember results bootstrapped simply reliable. , let us see bootstrapping give us.  bootstrapped GAM results tell rather different story OLS results. case, bootstrap confidence bands variables (even complaints) fully overlap entirety median non-significance region. Even average slopes vanished variables except complaint, remains positive, yet insignificant wide confidence interval. , conclude? First, tempting retain OLS results tell interesting story. consider irresponsible since GAM model clearly superior terms adjusted R2: model far reliably tells us really going . tell us? seems positive effect handled complaints ratings (higher percentage complaints handled, higher average rating), data allow us sufficiently certain generalize results. insufficient evidence variables effect . doubt, inconclusive results dataset small (30 rows). dataset even double size might show significant effects least complaints, variables.","code":"gam_attitude <- mgcv::gam(rating ~ complaints + privileges + s(learning) +                             raises + s(critical) + advance,                           data = attitude) summary(gam_attitude) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> rating ~ complaints + privileges + s(learning) + raises + s(critical) +  #>     advance #>  #> Parametric coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept) 36.97245   11.60967   3.185 0.004501 **  #> complaints   0.60933    0.13297   4.582 0.000165 *** #> privileges  -0.12662    0.11432  -1.108 0.280715     #> raises       0.06222    0.18900   0.329 0.745314     #> advance     -0.23790    0.14807  -1.607 0.123198     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Approximate significance of smooth terms: #>               edf Ref.df     F p-value   #> s(learning) 1.923  2.369 3.761  0.0312 * #> s(critical) 2.296  2.862 3.272  0.0565 . #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> R-sq.(adj) =  0.776   Deviance explained = 83.9% #> GCV = 47.947  Scale est. = 33.213    n = 30 ale_gam_attitude_simple <- ale(attitude, gam_attitude) #> Calculating ALE ■                                  0% |  ETA: ?  gridExtra::grid.arrange(grobs = ale_gam_attitude_simple$plots, ncol = 2) mb_gam <- model_bootstrap(   attitude,    gam_attitude,    boot_it = 10,  # 100 by default but reduced here for a faster demonstration   silent = TRUE  # progress bars disabled for the vignette   ) mb_gam$model_stats #> # A tibble: 3 × 7 #>   name        estimate conf.low  mean median conf.high    sd #>   <chr>          <dbl>    <dbl> <dbl>  <dbl>     <dbl> <dbl> #> 1 df              14.4     8.18  14.4   14.5      20.5  4.62 #> 2 df.residual     15.6     9.45  15.6   15.5      21.8  4.62 #> 3 nobs            30      30     30     30        30    0 mb_gam$model_coefs #> # A tibble: 2 × 7 #>   term        estimate conf.low  mean median conf.high std.error #>   <chr>          <dbl>    <dbl> <dbl>  <dbl>     <dbl>     <dbl> #> 1 s(learning)     5.17     1.20  5.17   4.78      8.99      3.58 #> 2 s(critical)     4.24     1.26  4.24   4.84      6.94      2.26 gridExtra::grid.arrange(grobs = mb_gam$ale$plots, ncol = 2)"},{"path":"https://tripartio.github.io/ale/articles/ale-small-datasets.html","id":"model_call_string-argument-for-non-standard-models","dir":"Articles","previous_headings":"","what":"model_call_string argument for non-standard models","title":"Analyzing small datasets (<2000 rows) with ALE","text":"[model_bootstrap()] accesses model object internally modifies retrain model bootstrapped datasets. able automatically manipulate R model objects used statistical analysis. However, object follow standard conventions R model objects, [model_bootstrap()] might able manipulate . , function fail early appropriate error message. case, user must specify model_call_string argument character string full call model boot_data data argument call. (boot_data placeholder bootstrap datasets [model_bootstrap()] internally work .) show works, let’s pretend mgcv::gam object needs special treatment. construct, model_call_string, must first execute model make sure works. earlier repeat demonstration ’re sure model call works, model_call_string constructed three simple steps: Wrap entire call (everything right assignment operator <-) quotes. Replace dataset data argument boot_data. Pass quoted string [model_bootstrap()] model_call_string argument (argument must explicitly named). , form call [model_bootstrap()] non-standard model object type: Everything else works normal.","code":"gam_attitude_again <- mgcv::gam(rating ~ complaints + privileges + s(learning) +                                   raises + s(critical) + advance,                                 data = attitude) summary(gam_attitude_again) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> rating ~ complaints + privileges + s(learning) + raises + s(critical) +  #>     advance #>  #> Parametric coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept) 36.97245   11.60967   3.185 0.004501 **  #> complaints   0.60933    0.13297   4.582 0.000165 *** #> privileges  -0.12662    0.11432  -1.108 0.280715     #> raises       0.06222    0.18900   0.329 0.745314     #> advance     -0.23790    0.14807  -1.607 0.123198     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Approximate significance of smooth terms: #>               edf Ref.df     F p-value   #> s(learning) 1.923  2.369 3.761  0.0312 * #> s(critical) 2.296  2.862 3.272  0.0565 . #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> R-sq.(adj) =  0.776   Deviance explained = 83.9% #> GCV = 47.947  Scale est. = 33.213    n = 30 mb_gam_non_standard <- model_bootstrap(   attitude,   model_call_string = 'mgcv::gam(rating ~ complaints + privileges + s(learning) +                                   raises + s(critical) + advance,                                 data = boot_data)',    boot_it = 10,  # 100 by default but reduced here for a faster demonstration   silent = TRUE  # progress bars disabled for the vignette   ) mb_gam_non_standard$model_stats #> # A tibble: 3 × 7 #>   name        estimate conf.low  mean median conf.high    sd #>   <chr>          <dbl>    <dbl> <dbl>  <dbl>     <dbl> <dbl> #> 1 df              14.4     8.18  14.4   14.5      20.5  4.62 #> 2 df.residual     15.6     9.45  15.6   15.5      21.8  4.62 #> 3 nobs            30      30     30     30        30    0"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"example-dataset","dir":"Articles","previous_headings":"","what":"Example dataset","title":"ALE-based statistics for statistical inference and effect sizes","text":"demonstrate ALE statistics using dataset composed transformed mgcv package. package required create generalized additive model (GAM) use demonstration. (Strictly speaking, source datasets nlme package, loaded automatically load mgcv package.) code generate data work : structure 160 rows, refers school whose students taken mathematics achievement test. describe data based documentation nlme package many details quite clear: particular note variable rand_norm. added completely random variable (normal distribution) demonstrate randomness looks like analysis. outcome variable focus analysis math_avg, average mathematics achievement scores students school. descriptive statistics:","code":"# Create and prepare the data  # Specific seed chosen to illustrate the spuriousness of the random variable set.seed(0)    math <-    # Start with math achievement scores per student   MathAchieve |>    as_tibble() |>    mutate(     school = School |> as.character() |>  as.integer(),     minority = Minority == 'Yes',     female = Sex == 'Female'   ) |>    # summarize the scores to give per-school values   summarize(     .by = school,     minority_ratio = mean(minority),     female_ratio = mean(female),     math_avg = mean(MathAch),   ) |>    # merge the summarized student data with the school data   inner_join(     MathAchSchool |>        mutate(school = School |> as.character() |>  as.integer()),     by = c('school' = 'school')   ) |>    mutate(     public = Sector == 'Public',     high_minority = HIMINTY == 1,   ) |>    select(-School, -Sector, -HIMINTY) |>    rename(     size = Size,     academic_ratio = PRACAD,     discrim = DISCLIM,     mean_ses = MEANSES,   ) |>    # Remove ID column for analysis   select(-school) |>    select(     math_avg, size, public, academic_ratio,     female_ratio, mean_ses, minority_ratio, high_minority, discrim,     everything()   ) |>    mutate(     rand_norm = rnorm(nrow(MathAchSchool))    )  glimpse(math) #> Rows: 160 #> Columns: 10 #> $ math_avg       <dbl> 9.715447, 13.510800, 7.635958, 16.255500, 13.177687, 11… #> $ size           <dbl> 842, 1855, 1719, 716, 455, 1430, 2400, 899, 185, 1672, … #> $ public         <lgl> TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALS… #> $ academic_ratio <dbl> 0.35, 0.27, 0.32, 0.96, 0.95, 0.25, 0.50, 0.96, 1.00, 0… #> $ female_ratio   <dbl> 0.5957447, 0.4400000, 0.6458333, 0.0000000, 1.0000000, … #> $ mean_ses       <dbl> -0.428, 0.128, -0.420, 0.534, 0.351, -0.014, -0.007, 0.… #> $ minority_ratio <dbl> 0.08510638, 0.12000000, 0.97916667, 0.40000000, 0.72916… #> $ high_minority  <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F… #> $ discrim        <dbl> 1.597, 0.174, -0.137, -0.622, -1.694, 1.535, 2.016, -0.… #> $ rand_norm      <dbl> 1.262954285, -0.326233361, 1.329799263, 1.272429321, 0.… summary(math$math_avg) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    4.24   10.47   12.90   12.62   14.65   19.72"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"full-model-bootstrap","dir":"Articles","previous_headings":"","what":"Full model bootstrap","title":"ALE-based statistics for statistical inference and effect sizes","text":"Now create model compute statistics . relatively small dataset, carry full model bootstrapping using [model_bootstrap()] function. create generalized additive model (GAM) can capture non-linear relationships data. default, [model_bootstrap()] runs 100 bootstrap iterations; can controlled boot_it argument. Bootstrapping usually rather slow, even small datasets, since entire process repeated many times. default 100 sufficiently stable model building, want run bootstrapped algorithm several times want slow time. definitive conclusions, run 1,000 bootstraps confirm results 100 bootstraps. can see bootstrapped values various overall model statistics printing model_stats element model bootstrap object: names columns follow broom package conventions: name specific overall model statistic described row. estimate bootstrapped estimate statistic. bootstrap mean default, though can set median boot_centre argument [model_bootstrap()]. Regardless, mean median estimates always returned. estimate column provided convenience since standard name broom package. conf.low conf.high lower upper confidence intervals respectively. [model_bootstrap()] defaults 95% confidence interval; can changed setting boot_alpha argument (default 0.05 95% confidence interval). sd standard deviation bootstrapped estimate. focus, however, vignette effects individual variables. available model_coefs element model bootstrap object: vignette, go details GAM models work (can learn Noam Ross’s excellent tutorial). However, model illustration , estimates parametric variables (non-numeric ones model) interpreted regular statistical regression coefficients whereas estimates non-parametric smoothed variables (whose variable names encapsulated smooth s() function) actually estimates expected degrees freedom (EDF GAM). smooth function s() lets GAM model numeric variables flexible curves fit data better straight line. estimate values smooth variables straightforward interpret, suffice say completely different regular regression coefficients. ale package uses bootstrap-based confidence intervals, p-values, determine statistical significance. Although quite simple interpret counting number stars next p-value, complicated, either. Based default 95% confidence intervals, coefficient statistically significant conf.low conf.high positive negative. can filter results criterion: statistical significance estimate (EDF) smooth terms meaningless EDF go 1.0. Thus, even random term s(rand_norm) appears “statistically significant”. values non-smooth (parametric terms) public high_minority considered . , find neither coefficient estimates public high_minority effect statistically significantly different zero. (intercept conceptually meaningful ; statistical artifact.) initial analysis highlights two limitations classical hypothesis-testing analysis. First, might work suitably well use models traditional linear regression coefficients. use advanced models like GAM flexibly fit data, interpret coefficients meaningfully clear reach inferential conclusions. Second, basic challenge models based general linear model (including GAM almost statistical analyses) coefficient significance compares estimates null hypothesis effect. However, even effect, might practically meaningful. see, ALE-based statistics explicitly tailored emphasize practical implications beyond notion “statistical significance”.","code":"gam_math <- gam(      math_avg ~ public + high_minority +      s(size) + s(academic_ratio) + s(female_ratio) + s(mean_ses) +       s(minority_ratio) + s(discrim) + s(rand_norm),      data = math    )  mb_gam <- model_bootstrap(   math,    gam_math,   # For the GAM model coefficients, show details of all variables, parametric or not   tidy_options = list(parametric = TRUE),   # tidy_options = list(parametric = NULL),   boot_it = 40,  # 100 by default but reduced here for a faster demonstration   silent = TRUE  # progress bars disabled for the vignette ) mb_gam$model_stats #> # A tibble: 3 × 7 #>   name        estimate conf.low  mean median conf.high    sd #>   <chr>          <dbl>    <dbl> <dbl>  <dbl>     <dbl> <dbl> #> 1 df              39.0     24.4  39.0   38.4      57.0  8.80 #> 2 df.residual    121.     103.  121.   122.      136.   8.80 #> 3 nobs           160      160   160    160       160    0 mb_gam$model_coefs #> # A tibble: 3 × 7 #>   term              estimate conf.low   mean median conf.high std.error #>   <chr>                <dbl>    <dbl>  <dbl>  <dbl>     <dbl>     <dbl> #> 1 (Intercept)         12.7    11.9    12.7   12.7      13.6       0.511 #> 2 publicTRUE          -0.724  -1.80   -0.724 -0.749     0.218     0.661 #> 3 high_minorityTRUE    1.13   -0.0527  1.13   1.10      2.53      0.741 mb_gam$model_coefs |>    # filter is TRUE if conf.low and conf.high are both positive or both negative because   # multiplying two numbers of the same sign results in a positive number.   filter((conf.low * conf.high) > 0) #> # A tibble: 1 × 7 #>   term        estimate conf.low  mean median conf.high std.error #>   <chr>          <dbl>    <dbl> <dbl>  <dbl>     <dbl>     <dbl> #> 1 (Intercept)     12.7     11.9  12.7   12.7      13.6     0.511"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"ale-effect-size-measures","dir":"Articles","previous_headings":"","what":"ALE effect size measures","title":"ALE-based statistics for statistical inference and effect sizes","text":"ALE developed graphically display relationship predictor variables model outcome regardless nature model. Thus, proceed describe extension effect size measures based ALE, let us first briefly examine ALE plots variable.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"ale-plots","dir":"Articles","previous_headings":"ALE effect size measures","what":"ALE plots","title":"ALE-based statistics for statistical inference and effect sizes","text":"can see variables seem sort mean effect across various values. However, statistical inference, focus must bootstrap intervals. Crucial interpretation middle grey band indicates median ± 2.5%, , middle 5% average mathematics achievement scores (math_avg) values dataset. call “median band”. idea predictor can better influencing math_avg fall within middle median band, minimal effect. effect considered statistically significant, overlap confidence regions predictor variable median band. (use 5% default, value can changed median_band argument.) categorical variables (public high_minority ), confidence interval bars categories overlap median band. confidence interval bars indicate two useful pieces information us. compare median band, overlap lack thereof tells us practical significance category. compare confidence bars one category others, allows us assess category statistically significant effect different categories; equivalent regular interpretation coefficients GAM GLM models. cases, confidence interval bars TRUE FALSE categories overlap , indicating statistically significant difference categories. Whereas coefficient table based classic statistics indicated conclusion public, indicated high_minority statistically significant effect; ALE analysis indicates high_minority . addition, confidence interval band overlaps median band, indicating none effects practically significant, either. numeric variables, confidence regions overlap median band domains predictor variables except regions examine. extreme points variable (except discrim female_ratio) usually either slightly slightly median band, indicating extreme values extreme effects: math achievement increases increasing school size, academic track ratio, mean socioeconomic status, whereas decreases increasing minority ratio. ratio females discrimination climate overlap median band entirety domains, apparent trends supported data. particular interest random variable rand_norm, whose average ALE appears show sort pattern. However, can see confidence intervals overlap median band entire domain. return implications observation.","code":"gridExtra::grid.arrange(grobs = mb_gam$ale$plots, ncol = 2)"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"ale-effect-size-measures-on-the-scale-of-the-y-outcome-variable","dir":"Articles","previous_headings":"ALE effect size measures","what":"ALE effect size measures on the scale of the y outcome variable","title":"ALE-based statistics for statistical inference and effect sizes","text":"Although ALE plots allow rapid intuitive conclusions statistical inference, often helpful summary numbers quantify average strengths effects variable. Thus, developed collection effect size measures based ALE tailored intuitive interpretation. understand intuition underlying various ALE effect size measures, useful first examine ALE effects plot, graphically summarizes effect sizes variables ALE analysis. generated ale executed statistics plots requested (case default) accessible focus measures specific variable, can access ale$stats$effects_plot element:  plot unusual, requires explanation: y (vertical) axis displays x variables, rather x axis. consistent effect size plots list full names variables. readable list labels y axis way around. x (horizontal) axis thus displays y (outcome) variable. two representations axis, one bottom one top. bottom typical axis outcome variable, case, math_avg. scaled expected. case, axis breaks default five units 5 20, evenly spaced. top, outcome variable expressed percentiles ranging 0 (minimum outcome value dataset) 100 (maximum). divided 10 deciles 10% . percentiles usually evenly distributed dataset, decile breaks evenly spaced. Thus, plot two x axes, lower one units outcome variable upper one percentiles outcome variable. reduce confusion, major vertical gridlines slightly darker align units outcome (lower axis) minor vertical gridlines slightly lighter align percentiles (upper axis). vertical grey band middle median band, representing median ± 2.5% values. variables horizontal axis sorted decreasing NALED value (explained ). NALED universal ALE measure effect size. Although somewhat confusing two axes, percentiles direct transformation raw outcome values. first two base ALE effect size measures units outcome variable normalized versions percentiles outcome. Thus, plot can display two kinds measures simultaneously. Referring plot can help understand measures, proceed explain detail. explain measures detail, must reiterate timeless reminder correlation causation. , none scores necessarily means x variable causes certain effect y outcome; can say ALE effect size measures indicate associated related variations two variables.","code":"mb_gam$ale$stats$effects_plot"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"ale-range-aler","dir":"Articles","previous_headings":"ALE effect size measures > ALE effect size measures on the scale of the y outcome variable","what":"ALE range (ALER)","title":"ALE-based statistics for statistical inference and effect sizes","text":"easiest ALE statistic understand ALE range (ALER), begin . simply range minimum maximum ale_y value variable. Mathematically, \\[\\mathrm{ALER}(\\mathrm{ale\\_y}) = \\{ \\min(\\mathrm{ale\\_y}), \\max(\\mathrm{ale\\_y}) \\}\\] \\(\\mathrm{ale\\_y}\\) vector ALE y values variable. ALE effect size measures centred zero consistent regardless user chooses centre plots zero, median, mean. Specifically, aler_min: minimum ale_y value variable. aler_max: maximum ale_y value variable. ALER shows extreme values variable’s effect outcome. effects plot , indicated extreme ends horizontal bars variable. can access ALE effect size measures ale$stats element bootstrap result object, multiple views. focus measures specific variable, can access ale$stats$by_term element. effect size measures categorical public: see public ALER -0.35, 0.43. consider median math score dataset 12.9, ALER indicates minimum ALE y value public (public == TRUE) -0.35 median. shown 12.5 mark plot . maximum (public == FALSE) 0.43 median, shown 13.3 point . unit ALER unit outcome variable; case, math_avg ranging 2 20. matter average ALE values might , ALER quickly shows minimum maximum effects value x variable y variable. Now, ALE effect size measures numeric academic_ratio: ALER academic_ratio considerably broader -3.69 1.74 median.","code":"mb_gam$ale$stats$by_term$public #> # A tibble: 6 × 6 #>   statistic estimate conf.low median   mean conf.high #>   <chr>        <dbl>    <dbl>  <dbl>  <dbl>     <dbl> #> 1 aled         0.382   0.0316  0.367  0.382    0.887  #> 2 aler_min    -0.347  -0.846  -0.311 -0.347   -0.0348 #> 3 aler_max     0.427   0.0292  0.394  0.427    1.07   #> 4 naled        5.36    1.04    5.04   5.36    12.5    #> 5 naler_min   -4.41  -10.1    -3.73  -4.41     0      #> 6 naler_max    6.52    0.609   5.62   6.52    17.0 mb_gam$ale$stats$by_term$academic_ratio #> # A tibble: 6 × 6 #>   statistic estimate conf.low  median    mean conf.high #>   <chr>        <dbl>    <dbl>   <dbl>   <dbl>     <dbl> #> 1 aled         0.602    0.312   0.591   0.602     0.880 #> 2 aler_min    -3.69    -6.72   -4.23   -3.69     -0.800 #> 3 aler_max     1.74     0.841   1.49    1.74      3.37  #> 4 naled        7.95     4.08    7.72    7.95     12.5   #> 5 naler_min  -32.4    -47.6   -37.4   -32.4      -5.56  #> 6 naler_max   23.9     12.4    21.0    23.9      41.3"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"ale-deviation-aled","dir":"Articles","previous_headings":"ALE effect size measures > ALE effect size measures on the scale of the y outcome variable","what":"ALE deviation (ALED)","title":"ALE-based statistics for statistical inference and effect sizes","text":"ALE range shows extreme effects variable might outcome, ALE deviation indicates average effect full domain values. zero-centred ALE values, conceptually similar weighted mean absolute error (MAE) ALE y values. Mathematically, \\[ \\mathrm{ALED}(\\mathrm{ale\\_y}, \\mathrm{ale\\_n}) = \\frac{\\sum_{=1}^{k} \\left| \\mathrm{ale\\_y}_i \\times \\mathrm{ale\\_n}_i \\right|}{\\sum_{=1}^{k} \\mathrm{ale\\_n}_i} \\] \\(\\) index \\(k\\) ALE x intervals variable (categorical variable, number distinct categories), \\(\\mathrm{ale\\_y}_i\\) ALE y value \\(\\)th ALE x interval, \\(\\mathrm{ale\\_n}_i\\) number rows data \\(\\)th ALE x interval. Based ALED, can say average effect math scores whether school public Catholic sector 0.38 (, range 2 20). effects plot , ALED indicated white box bounded parentheses ( ). centred median, can readily see average effect school sector barely exceeds limits median band, indicating barely exceeds threshold practical relevance. average effect ratio academic track students slightly higher 0.6. can see plot slightly exceeds median band sides, indicating slightly stronger effect. comment values variables discuss normalized versions scores, proceed next.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"normalized-ale-effect-size-measures","dir":"Articles","previous_headings":"ALE effect size measures","what":"Normalized ALE effect size measures","title":"ALE-based statistics for statistical inference and effect sizes","text":"Since ALER ALED scores scaled range y given dataset, scores compared across datasets. Thus, present normalized versions intuitive, comparable values. intuitive interpretation, normalize scores minimum, median, maximum dataset. principle, divide zero-centred y values dataset two halves: lower half 0th 50th percentile (median) upper half 50th 100th percentile. (Note median included halves). zero-centred ALE y values, negative zero values converted percentile score relative lower half original y values positive ALE y values converted percentile score relative upper half. (Technically, percentile assignment called empirical cumulative distribution function (ECDF) half.) half divided two scale 0 50 together can represent 100 percentiles. (Note: centred ALE y value exactly 0 occurs, choose include score zero ALE y lower half analogous 50th percentile values, intuitively belongs lower half 100 percentiles.) transformed maximum ALE y scaled percentile 0 100%. notable complication. normalization smoothly distributes ALE y values many distinct values, distinct ALE y values, even minimal ALE y deviation can relatively large percentile difference. ALE y value less difference median data value either immediately median, consider virtually effect. Thus, normalization sets minimal ALE y values zero. formula : \\[ norm\\_ale\\_y = 100 \\times \\begin{cases} 0 & \\text{} \\max(centred\\_y < 0) \\leq ale\\_y \\leq \\min(centred\\_y > 0), \\\\ \\frac{-ECDF_{y_{\\leq 0}}(ale\\_y)}{2} & \\text{}ale\\_y < 0 \\\\ \\frac{ECDF_{y_{\\geq 0}}(ale\\_y)}{2} & \\text{}ale\\_y > 0 \\\\ \\end{cases} \\] - \\(centred\\_y\\) vector y values centred median (, median subtracted values). - \\(ECDF_{y_{\\geq 0}}\\) ECDF non-negative values y. - \\(-ECDF_{y_{\\leq 0}}\\) ECDF negative values y inverted (multiplied -1). course, formula simplified multiplying 50 instead 100 dividing ECDFs two . prefer form given explicit ECDF represents half percentile range result scored 100 percentiles.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"normalized-aler-naler","dir":"Articles","previous_headings":"ALE effect size measures > Normalized ALE effect size measures","what":"Normalized ALER (NALER)","title":"ALE-based statistics for statistical inference and effect sizes","text":"Based normalization, first normalized ALER (NALER), scales minimum maximum ALE y values -50% +50%, centred 0%, represents median: \\[ \\mathrm{NALER}(\\mathrm{y, ale\\_y}) = \\{\\min(\\mathrm{norm\\_ale\\_y}) + 50, \\max(\\mathrm{norm\\_ale\\_y}) + 50 \\} \\] \\(y\\) full vector y values original dataset, required calculate \\(\\mathrm{norm\\_ale\\_y}\\). ALER shows extreme values variable’s effect outcome. effects plot , indicated extreme ends horizontal bars variable. see public ALER -0.35, 0.43. consider median math score dataset 12.9, ALER indicates minimum ALE y value public (public == TRUE) -0.35 median. shown 12.5 mark plot . maximum (public == FALSE) 0.43 median, shown 13.3 point . ALER academic_ratio considerably broader -3.69 1.74 median. result transformation NALER values can interpreted percentile effects y median, centred 0%. numbers represent limits effect x variable units percentile scores y. effects plot , percentile scale top corresponds exactly raw scale , NALER limits represented exactly points ALER limits; scale changes. scale ALER ALED lower scale raw outcomes; scale NALER NALED upper scale percentiles. , NALER -4.41, 6.52, minimum ALE value public (public == TRUE) shifts math scores -4 percentile y points whereas maximum (public == FALSE) shifts math scores 7 percentile points. Academic track ratio NALER -32.37, 23.92, ranging -32 24 percentile points math scores.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"normalized-aled-naled","dir":"Articles","previous_headings":"ALE effect size measures > Normalized ALE effect size measures","what":"Normalized ALED (NALED)","title":"ALE-based statistics for statistical inference and effect sizes","text":"normalization ALED scores applies ALED formula normalized ALE values instead original ALE y values: \\[ \\mathrm{NALED}(y, \\mathrm{ale\\_y}, \\mathrm{ale\\_n}) = \\mathrm{ALED}(\\mathrm{norm\\_ale\\_y}, \\mathrm{ale\\_n}) \\] NALED produces score ranges 0 100%. essentially ALED expressed percentiles, , average effect variable full domain values. , NALED public school status 5.4 indicates average effect math scores spans middle 5.4 percent scores. Academic ratio average effect expressed NALED 8% scores. NALED particularly helpful comparing practical relevance variables threshold consider variable needs shift outcome average 5% median values. threshold scale NALED. , can tell public school status NALED 5.4 just barely crosses threshold.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"the-median-band-and-random-variables","dir":"Articles","previous_headings":"ALE effect size measures","what":"The median band and random variables","title":"ALE-based statistics for statistical inference and effect sizes","text":"particularly striking note ALE effect size measures random rand_norm: rand_norm NALED 2.7. might surprising purely random value “effect size” speak , statistically, must numeric value . However, setting default value median band 5%, effectively exclude rand_norm serious consideration. Setting median band low value like 1% excluded random variable, 5% seems like nice balance. Thus, effect variable like discrimination climate score (discrim, 3.9) probably considered practically meaningful. one hand, 5% threshold median band might seem somewhat arbitrary, inspired traditional \\(\\alpha\\) = 0.05 statistical significance confidence intervals. “correct” baseline qualitative question, depending analyst’s goals context specific study. hand, initial analyses show 5% seems effective choice excluding purely random variable consideration, whether small large datasets.","code":"mb_gam$ale$stats$by_term$rand_norm #> # A tibble: 6 × 6 #>   statistic estimate  conf.low median   mean conf.high #>   <chr>        <dbl>     <dbl>  <dbl>  <dbl>     <dbl> #> 1 aled         0.177   0.00676  0.131  0.177    0.427  #> 2 aler_min    -0.824  -2.55    -0.507 -0.824   -0.0204 #> 3 aler_max     0.581   0.0227   0.322  0.581    2.27   #> 4 naled        2.66    0.00455  2.04   2.66     6.35   #> 5 naler_min   -9.66  -30.6     -5.62  -9.66     0      #> 6 naler_max    8.21    0        3.73   8.21    30.4"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"interpretation-of-normalized-ale-effect-sizes","dir":"Articles","previous_headings":"ALE effect size measures","what":"Interpretation of normalized ALE effect sizes","title":"ALE-based statistics for statistical inference and effect sizes","text":"summarize general principles interpreting normalized ALE effect sizes. 0% means effect . 100% means maximum possible effect variable : binary variable, one value (50% data) sets outcome minimum value value (50% data) sets outcome maximum value. Larger NALED means stronger effects. NOTE: consider NALER ALE plot see extreme values strong effect, even vast majority values meaningful effect. caveat important; unlike GLM coefficients, ALE analysis sensitive exceptions overall trend. precisely makes valuable detecting non-linear effects. NALER minimum ranges –50% 0%; NALER maximum ranges 0% +50%: 0% means effect . indicates effect input variable keep outcome median range values. NALER minimum n means , regardless effect size NALED, minimum effect input value shifts outcome n percentile points outcome range. Lower values (closer –50%) mean stronger extreme effect. NALER maximum x means , regardless effect size NALED, maximum effect input value shifts outcome x percentile points outcome range. Greater values (closer +50%) mean stronger extreme effect. general, regardless values ALE statistics, always visually inspect ALE plots identify interpret patterns relationships inputs outcome. However, many variables analysis procedures must make automated decisions based strength ALE effects, helpful consider rules thumb might generally considered meaningful ALE effect size measures. Regardless average effect indicated NALED, large NALER effects indicate ALE plot inspected interpret exceptional cases. general, NALED < 5%, NALER minimum > –5%, NALER maximum < +5%, input variable meaningful effect. cases worth inspecting ALE plots careful interpretation: - NALED > 5% means meaningful average effect. - NALER minimum < –5% means might least one input value significantly lowers outcome values. - NALER maximum > +5% means might least one input value significantly increases outcome values.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"statistical-inference-with-ale","dir":"Articles","previous_headings":"","what":"Statistical inference with ALE","title":"ALE-based statistics for statistical inference and effect sizes","text":"Although effect sizes valuable summarizing global effects variable, mask much nuance since variable varies effect along domain values. Thus, ALE particularly powerful ability make fine-grained inferences variable’s effect depending specific value.","code":""},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"ale-data-structures-for-categorical-and-numeric-variables","dir":"Articles","previous_headings":"Statistical inference with ALE","what":"ALE data structures for categorical and numeric variables","title":"ALE-based statistics for statistical inference and effect sizes","text":"understand bootstrapped ALE can used statistical inference, must understand structure ALE data. Let’s begin simple binary variable just two categories, public: meaning column ale$data categorical variable: ale_x: different categories exist categorical variable. ale_n: number rows category dataset provided function. ale_y: ALE function value calculated category. bootstrapped ALE, ale_y_mean default ale_y_median relative_y = 'median' argument specified. ale_y_lo ale_y_hi: lower upper confidence intervals bootstrapped ale_y value. default, ale package centres ALE values median outcome variable; dataset, median schools’ average mathematics achievement scores 12.9. ALE centred median, weighted sum ALE y values (weighted ale_n) median approximately equal weighted sum median. , ALE plots , consider number instances indicated rug plots category percentages, average weighted ALE y approximately equals median. ALE data structure numeric variable, academic_ratio: columns categorical variable, meaning ale_x different since categories. calculate ALE numeric variables, range x values divided fixed intervals (default 100, customizable x_intervals argument). x values fewer 100 distinct values data, distinct value becomes ale_x interval. (often case smaller datasets like ; academic_ratio 65 distinct values.) 100 distinct values, range divided 100 percentile groups. , ale_x represents x-variable intervals. columns mean thing categorical variables: ale_n number rows data ale_x interval ale_y calculated ALE ale_x value.","code":"mb_gam$ale$data$public #> # A tibble: 2 × 7 #>   ale_x ale_n ale_y ale_y_lo ale_y_mean ale_y_median ale_y_hi #>   <ord> <int> <dbl>    <dbl>      <dbl>        <dbl>    <dbl> #> 1 FALSE    70  13.3     12.5       13.3         13.3     14.3 #> 2 TRUE     90  12.6     12.0       12.6         12.7     13.2 mb_gam$ale$data$academic_ratio #> # A tibble: 65 × 7 #>    ale_x ale_n ale_y ale_y_lo ale_y_mean ale_y_median ale_y_hi #>    <dbl> <int> <dbl>    <dbl>      <dbl>        <dbl>    <dbl> #>  1  0        1  9.23     6.13       9.23         8.80     12.2 #>  2  0.05     2 11.0      9.40      11.0         11.1      13.0 #>  3  0.09     1 11.8     11.0       11.8         11.7      13.0 #>  4  0.1      2 12.0     11.3       12.0         11.9      13.3 #>  5  0.13     1 12.4     11.4       12.4         12.3      14.3 #>  6  0.14     2 12.3     11.4       12.3         12.2      14.1 #>  7  0.17     1 12.3     11.6       12.3         12.2      13.5 #>  8  0.18     4 12.4     11.7       12.4         12.3      14.1 #>  9  0.19     3 12.4     11.6       12.4         12.3      13.8 #> 10  0.2      3 12.5     11.7       12.5         12.4      13.7 #> # ℹ 55 more rows"},{"path":"https://tripartio.github.io/ale/articles/ale-statistics.html","id":"bootstrap-based-inference-with-ale","dir":"Articles","previous_headings":"Statistical inference with ALE","what":"Bootstrap-based inference with ALE","title":"ALE-based statistics for statistical inference and effect sizes","text":"bootstrapped ALE plot, values within confidence intervals statistically significant; values outside median band can considered least somewhat meaningful. Thus, essence ALE-based statistical inference effects simultaneously within confidence intervals outside median band considered conceptually meaningful. can see , example, plot academic_ratio:  might always easy tell plot regions relevant, results statistical significance summarized ale$conf_regions element, can accessed variable: numeric variables, confidence regions summary one row consecutive sequence x values status: values region middle irrelevance band, overlap band, band. summary components: start_x first end_x last x value sequence. start_y y value corresponds start_x end_y corresponds end_x. n number data elements sequence; n_pct percentage total data elements total number. x_span length x sequence confidence status. However, may comparable across variables different units x, x_span expressed percentage full domain x values. trend average slope point (start_x, start_y) (end_x, end_y). start end points used calculate trend, reflect ups downs might occur two points. Since various x values dataset different scales, scales x y values calculating trend normalized scale 100 trends variables directly comparable. positive trend means , average, y increases x; negative trend means , average, y decreases x; zero trend means y value start end points–always case one point indicated sequence. : higher limit confidence interval ALE y (ale_y_hi) lower limit median band. : lower limit confidence interval ALE y (ale_y_lo) higher limit median band. overlap: neither first two conditions holds; , confidence region ale_y_lo ale_y_hi least partially overlaps median band. results tell us simply , academic_ratio, 0 0, ALE median band 9.25 9.25. 0.05 0.9, ALE overlaps median band 11 13.7. 0.91 1, ALE median band 13.8 14.5. Interestingly, text previous paragraph generated automatically internal (unexported function) ale:::summarize_conf_regions_in_words. (Since function exported, must use ale::: three colons, just two, want access .) wording rather mechanical, nonetheless illustrates potential value able summarize inferentially relevant conclusions tabular form. Confidence region summary tables available numeric also categorical variables, see public. ALE plot:  confidence regions summary table: Since categories , start end positions trend. instead x category single ALE y value, n n_pct respective category relative_to_mid indicate whether indicated category , overlaps , median band. help ale:::summarize_conf_regions_in_words, results tell us , public, FALSE, ALE 13.3 overlaps median band. TRUE, ALE 12.6 overlaps median band. , random variable rand_norm particularly interesting. ALE plot:  confidence regions summary table: Despite apparent pattern, see -2.4 2.61, ALE overlaps median band 11.9 12.8. , despite random highs lows bootstrap confidence interval, reason suppose random variable effect anywhere domain.","code":"mb_gam$ale$plots$academic_ratio mb_gam$ale$conf_regions$academic_ratio #> # A tibble: 3 × 9 #>   start_x end_x x_span     n   n_pct start_y end_y trend relative_to_mid #>     <dbl> <dbl>  <dbl> <int>   <dbl>   <dbl> <dbl> <dbl> <ord>           #> 1    0     0    0          1 0.00625    9.23  9.23 0     below           #> 2    0.05  0.91 0.86     143 0.894     11.0  13.8  0.218 overlap         #> 3    0.95  1    0.0500    16 0.1       14.1  14.6  0.692 above ale:::summarize_conf_regions_in_words(mb_gam$ale$conf_regions$academic_ratio) #> [1] \"From 0 to 0, ALE is below the median band from 9.23 to 9.23. From 0.05 to 0.91, ALE overlaps the median band from 11 to 13.8. From 0.95 to 1, ALE is above the median band from 14.1 to 14.6.\" mb_gam$ale$plots$public mb_gam$ale$conf_regions$public #> # A tibble: 2 × 5 #>   x         n n_pct     y relative_to_mid #>   <ord> <int> <dbl> <dbl> <ord>           #> 1 FALSE    70 0.438  13.3 overlap         #> 2 TRUE     90 0.562  12.6 overlap mb_gam$ale$plots$rand_norm mb_gam$ale$conf_regions$rand_norm #> # A tibble: 1 × 9 #>   start_x end_x x_span     n n_pct start_y end_y    trend relative_to_mid #>     <dbl> <dbl>  <dbl> <int> <dbl>   <dbl> <dbl>    <dbl> <ord>           #> 1   -2.22  2.44      1   160     1    12.7  12.6 -0.00224 overlap"},{"path":"https://tripartio.github.io/ale/articles/ale-x-datatypes.html","id":"var_cars-modified-mtcars-dataset-motor-trend-car-road-tests","dir":"Articles","previous_headings":"","what":"var_cars: modified mtcars dataset (Motor Trend Car Road Tests)","title":"ale function handling of various datatypes for x","text":"demonstration, use modified version built-mtcars dataset binary (logical), multinomial (factor, , non-ordered categories), ordinal (ordered factor), discrete interval (integer), continuous interval (numeric double) values. modified version, called var_cars, let us test different basic variations x variables. factor, adds country car manufacturer. data tibble 32 observations 12 variables:","code":"print(var_cars) #> # A tibble: 32 × 12 #>      mpg   cyl  disp    hp  drat    wt  qsec vs    am    gear   carb country #>    <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl> <lgl> <lgl> <ord> <int> <fct>   #>  1  21       6  160    110  3.9   2.62  16.5 FALSE TRUE  four      4 Japan   #>  2  21       6  160    110  3.9   2.88  17.0 FALSE TRUE  four      4 Japan   #>  3  22.8     4  108     93  3.85  2.32  18.6 TRUE  TRUE  four      1 Japan   #>  4  21.4     6  258    110  3.08  3.22  19.4 TRUE  FALSE three     1 USA     #>  5  18.7     8  360    175  3.15  3.44  17.0 FALSE FALSE three     2 USA     #>  6  18.1     6  225    105  2.76  3.46  20.2 TRUE  FALSE three     1 USA     #>  7  14.3     8  360    245  3.21  3.57  15.8 FALSE FALSE three     4 USA     #>  8  24.4     4  147.    62  3.69  3.19  20   TRUE  FALSE four      2 Germany #>  9  22.8     4  141.    95  3.92  3.15  22.9 TRUE  FALSE four      2 Germany #> 10  19.2     6  168.   123  3.92  3.44  18.3 TRUE  FALSE four      4 Germany #> # ℹ 22 more rows summary(var_cars) #>       mpg             cyl             disp             hp        #>  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0   #>  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5   #>  Median :19.20   Median :6.000   Median :196.3   Median :123.0   #>  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7   #>  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0   #>  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0   #>       drat             wt             qsec           vs          #>  Min.   :2.760   Min.   :1.513   Min.   :14.50   Mode :logical   #>  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   FALSE:18        #>  Median :3.695   Median :3.325   Median :17.71   TRUE :14        #>  Mean   :3.597   Mean   :3.217   Mean   :17.85                   #>  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90                   #>  Max.   :4.930   Max.   :5.424   Max.   :22.90                   #>      am             gear         carb          country   #>  Mode :logical   three:15   Min.   :1.000   Germany: 8   #>  FALSE:19        four :12   1st Qu.:2.000   Italy  : 4   #>  TRUE :13        five : 5   Median :2.000   Japan  : 6   #>                             Mean   :2.812   Sweden : 1   #>                             3rd Qu.:4.000   UK     : 1   #>                             Max.   :8.000   USA    :12"},{"path":"https://tripartio.github.io/ale/articles/ale-x-datatypes.html","id":"modelling-with-ale-and-gam","dir":"Articles","previous_headings":"","what":"Modelling with ALE and GAM","title":"ale function handling of various datatypes for x","text":"GAM, numeric variables can smoothed, binary categorical ones. However, smoothing always help improve model since variables related outcome related actually simple linear relationship. keep demonstration simple, done earlier analysis (shown ) determines smoothing worthwhile modified var_cars dataset, numeric variables smoothed. goal demonstrate best modelling procedure rather demonstrate flexibility ale package. Now generate ALE data var_cars GAM model plot .  can see ale trouble modelling datatypes sample (logical, factor, ordered, integer, double). plots line charts numeric predictors column charts everything else. numeric predictors rug plots indicate ranges x (predictor) y (mpg) values data actually exists dataset. helps us -interpret regions data sparse. Since column charts discrete scale, rug plots. Instead, percentage data represented column displayed. can also generate plot ALE data two-way interactions.  interactions dataset. (see ALE interaction plots look like presence interactions, see ALEPlot comparison vignette, explains interaction plots detail.) Finally, explained vignette modelling small datasets, appropriate modelling workflow require bootstrapping entire model, just ALE data. , let’s now.  (default, [model_bootstrap()] creates 100 bootstrap samples , illustration runs faster, demonstrate 10 iterations.) small dataset, bootstrap confidence interval always overlap middle band, indicating dataset support claims variables meaningful effect fuel efficiency (mpg). Considering average bootstrapped ALE values suggest various intriguing patterns, problem doubt dataset small–data collected analyzed, patterns probably confirmed.","code":"cm <- mgcv::gam(mpg ~ cyl + disp + hp + drat + wt + s(qsec) +                   vs + am + gear + carb + country,                 data = var_cars) summary(cm) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> mpg ~ cyl + disp + hp + drat + wt + s(qsec) + vs + am + gear +  #>     carb + country #>  #> Parametric coefficients: #>               Estimate Std. Error t value Pr(>|t|)    #> (Intercept)   -7.84775   12.47080  -0.629  0.54628    #> cyl            1.66078    1.09449   1.517  0.16671    #> disp           0.06627    0.01861   3.561  0.00710 ** #> hp            -0.01241    0.02502  -0.496  0.63305    #> drat           4.54975    1.48971   3.054  0.01526 *  #> wt            -5.03737    1.53979  -3.271  0.01095 *  #> vsTRUE        12.45630    3.62342   3.438  0.00852 ** #> amTRUE         8.77813    2.67611   3.280  0.01080 *  #> gear.L         0.53111    3.03337   0.175  0.86525    #> gear.Q         0.57129    1.18201   0.483  0.64150    #> carb          -0.34479    0.78600  -0.439  0.67223    #> countryItaly  -0.08633    2.22316  -0.039  0.96995    #> countryJapan  -3.31948    2.22723  -1.490  0.17353    #> countrySweden -3.83437    2.74934  -1.395  0.19973    #> countryUK     -7.24222    3.81985  -1.896  0.09365 .  #> countryUSA    -7.69317    2.37998  -3.232  0.01162 *  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Approximate significance of smooth terms: #>           edf Ref.df     F p-value   #> s(qsec) 7.797  8.641 5.975  0.0101 * #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> R-sq.(adj) =  0.955   Deviance explained = 98.8% #> GCV = 6.4263  Scale est. = 1.6474    n = 32 cars_ale <- ale(var_cars, cm) #> Calculating ALE ■                                  0% |  ETA: ?  # Print all plots gridExtra::grid.arrange(grobs = cars_ale$plots, ncol = 2) cars_ale_ixn <- ale_ixn(var_cars, cm) #> Calculating ALE interactions ■                                  0% |  ETA: ? #> Calculating ALE interactions ■■■■■■■■■■■■■■■■■■■■■■■■■■■       86% |  ETA:  0s  # Print plots cars_ale_ixn$plots |>   purrr::walk(\\(.x1) {  # extract list of x1 ALE outputs     gridExtra::grid.arrange(grobs = .x1, ncol = 2)  # plot all x1 plots   }) mb <- model_bootstrap(   var_cars,    cm,   boot_it = 10,  # 100 by default but reduced here for a faster demonstration   silent = TRUE  # progress bars disabled for the vignette )  gridExtra::grid.arrange(grobs = mb$ale$plots, ncol = 2)"},{"path":"https://tripartio.github.io/ale/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Chitu Okoli. Author, maintainer. Dan Apley. Copyright holder.            current code calculating ALE interaction values copied changes Dan Apley's ALEPlot package. gratefully acknowledge open-source contribution. However, directly involved development ale package.","code":""},{"path":"https://tripartio.github.io/ale/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Okoli C (2023). “Statistical inference using machine learning classical techniques based accumulated local effects (ALE).” arXiv, 1-30. doi:10.48550/arXiv.2310.09877, https://arxiv.org/abs/2310.09877. Okoli C (2023). ale: Interpretable Machine Learning Statistical Inference Accumulated Local Effects (ALE). R package version 0.2.20240109, https://CRAN.R-project.org/package=ale.","code":"@Article{,   title = {Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)},   author = {Chitu Okoli},   year = {2023},   journal = {arXiv},   doi = {10.48550/arXiv.2310.09877},   url = {https://arxiv.org/abs/2310.09877},   pages = {1-30}, } @Manual{,   title = {ale: Interpretable Machine Learning and Statistical Inference with Accumulated Local Effects (ALE)},   author = {Chitu Okoli},   year = {2023},   note = {R package version 0.2.20240109},   url = {https://CRAN.R-project.org/package=ale}, }"},{"path":"https://tripartio.github.io/ale/index.html","id":"ale-package","dir":"","previous_headings":"","what":"Interpretable Machine Learning and Statistical Inference with Accumulated Local Effects (ALE)","title":"Interpretable Machine Learning and Statistical Inference with Accumulated Local Effects (ALE)","text":"Accumulated Local Effects (ALE) initially developed model-agnostic approach global explanations results black-box machine learning algorithms. ALE two primary advantages approaches like partial dependency plots (PDP) SHapley Additive exPlanations (SHAP): values affected presence interactions among variables model computation relatively rapid. package rewrites original code ‘ALEPlot’ package calculating ALE data completely reimplements plotting ALE values. also extends original ALE concept add bootstrap-based confidence intervals ALE-based statistics can used statistical inference. details, see Okoli, Chitu. 2023. “Statistical Inference Using Machine Learning Classical Techniques Based Accumulated Local Effects (ALE).” arXiv. https://doi.org/10.48550/arXiv.2310.09877. ale package replicates full functionality {ALEPlot} package lot . currently presents three main functions: ale: create data plot one-way ALE (single variables). ALE values may bootstrapped.   addition, minor functions helpful model evaluation. may find details following vignettes (available vignettes link main CRAN page https://CRAN.R-project.org/package=ale): Introduction ale package ALE-based statistics statistical inference effect sizes Analyzing small datasets (<2000 rows) ALE Comparison {ALEPlot} ale packages ale function handling various datatypes x","code":""},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and return ALE data, statistics, and plots — ale","title":"Create and return ALE data, statistics, and plots — ale","text":"ale() central function manages creation ALE data plots one-way ALE. two-way interactions, see ale_ixn(). function calls ale_core (non-exported function) manages ALE data plot creation detail. details, see introductory vignette package details examples . Custom predict function calculation ALE requires modifying several values original data. Thus, ale() needs direct access predict function work model. default, ale() uses generic default predict function form predict(object, newdata, type) default prediction type 'response'. , however, desired prediction values generated format, user must specify want. time, modification needed change prediction type value setting pred_type argument (e.g., 'prob' generated classification probabilities). desired predictions need different function signature, user must create custom prediction function pass pred_fun. requirements custom function : must take three required arguments nothing else: object: model newdata: dataframe compatible table type type: string; usually specified type = pred_type argument names according R convention generic stats::predict function. must return vector numeric values prediction. can see example custom prediction function. Note: survival models probably need custom prediction function y_col must set name binary event column pred_type must set desired prediction type. ALE statistics details ALE-based statistics (ALED, ALER, NALED, NALER), see vignette(\"ale-statistics\"). ale package Accumulated Local Effects (ALE) initially developed model-agnostic approach global explanations results black-box machine learning algorithms. ALE key advantage approaches like partial dependency plots (PDP) SHapley Additive exPlanations (SHAP): values represent clean functional decomposition model. , ALE values affected presence absence interactions among variables mode. Moreover, computation relatively rapid. package rewrites original code 'ALEPlot' package calculating ALE data completely reimplements plotting ALE values. also extends original ALE concept add bootstrap-based confidence intervals ALE-based statistics can used statistical inference. details, see Okoli, Chitu. 2023. “Statistical Inference Using Machine Learning Classical Techniques Based Accumulated Local Effects (ALE).” arXiv. https://arxiv.org/abs/2310.09877.","code":""},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and return ALE data, statistics, and plots — ale","text":"","code":"ale(   data,   model,   x_cols = NULL,   y_col = NULL,   ...,   output = c(\"plots\", \"data\", \"stats\"),   pred_fun = function(object, newdata, type = pred_type) {      stats::predict(object =     object, newdata = newdata, type = type)  },   pred_type = \"response\",   p_values = NULL,   x_intervals = 100,   boot_it = 0,   seed = 0,   boot_alpha = 0.05,   boot_centre = \"mean\",   relative_y = \"median\",   y_type = NULL,   median_band = c(0.05, 0.5),   rug_sample_size = 500,   min_rug_per_interval = 1,   ale_xs = NULL,   ale_ns = NULL,   silent = FALSE )"},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and return ALE data, statistics, and plots — ale","text":"data dataframe. Dataset create predictions ALE. model model object. Model ALE calculated. May kind R object can make predictions data. x_cols character. Vector column names data one-way ALE data calculated (, simple ALE without interactions). provided, ALE created columns data except y_col. y_col character length 1. Name outcome target label (y) variable. provided, ale() try detect automatically. non-standard models, y_col provided. survival models, set y_col name binary event column; case, pred_type also specified. ... used. Inserted require explicit naming subsequent arguments. output character c('plots', 'data', 'stats'). Vector types results return. 'plots' return ALE plot; 'data' return source ALE data; 'stats' return ALE statistics. option must listed return specified component. default, returned. pred_fun, pred_type function,character length 1. pred_fun function returns vector predicted values type pred_type model data. See details. p_values instructions calculating p-values. NULL (default), p-values calculated. calculate p-values, object generated create_p_funs() function must provided . p_values set 'auto', ale() function try automatically create p-values function; works standard R model types. error message given p-values generated. input provided argument result error. details creating p-values, see documentation create_p_funs(). Note p-values generated 'stats' included option output argument. x_intervals positive integer length 1. Maximum number intervals x-axis ALE data column x_cols. number intervals algorithm generates might eventually fewer user specifies data values given x value support many intervals. boot_it non-negative integer length 1. Number bootstrap iterations ALE values. boot_it = 0 (default), ALE calculated entire dataset bootstrapping. seed integer length 1. Random seed. Supply runs assure identical random ALE data generated time boot_alpha numeric length 1 0 1. Alpha percentile-based confidence interval range bootstrap intervals; bootstrap confidence intervals lowest highest (1 - 0.05) / 2 percentiles. example, boot_alpha = 0.05 (default), intervals 2.5 97.5 percentiles. boot_centre character length 1 c('mean', 'median'). bootstrapping, main estimate ale_y considered boot_centre. Regardless value specified , mean median available. relative_y character length 1 c('median', 'mean', 'zero'). ale_y values adjusted relative value. 'median' default. 'zero' maintain default ALEPlot::ALEPlot, shifted. y_type character length 1. Datatype y (outcome) variable. Must one c('binary', 'numeric', 'multinomial', 'ordinal'). Normally determined automatically; provide complex non-standard models require . median_band numeric length 2 0 1. Alpha \"confidence interval\" ranges printing bands around median single-variable plots. inner band range median value y ± median_band[1]. plots second outer band, range median ± median_band[2]. rug_sample_size, min_rug_per_interval single non-negative integer length 1. Rug plots normally -sampled otherwise slow. rug_sample_size specifies size sample. prevent -sampling, set Inf. suppress rug plots, set 0. -sampling, rug plots maintain representativeness data guaranteeing x_intervals intervals retain least min_rug_per_interval elements; usually set just 1 2. ale_xs, ale_ns list ale_x ale_n vectors. provided, vectors used set intervals ALE x axis variable. default (NULL), function automatically calculates ale_x intervals. ale_xs normally used advanced analyses ale_x intervals previous analysis reused subsequent analyses (example, full model bootstrapping; see model_bootstrap() function). silent logical length 1, default FALSE. TRUE, display non-essential messages execution (progress bars). Regardless, warnings errors always display.","code":""},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create and return ALE data, statistics, and plots — ale","text":"list elements data, plots, stats requested output argument. list named x variables respective values variable. requested output argument, return value NULL. addition, return object recapitulates several elements passed arguments apply x variables ALE calculation.","code":""},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create and return ALE data, statistics, and plots — ale","text":"ale_core.R Core functions ale package: ale, ale_ixn, ale_core","code":""},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create and return ALE data, statistics, and plots — ale","text":"Okoli, Chitu. 2023. “Statistical Inference Using Machine Learning Classical Techniques Based Accumulated Local Effects (ALE).” arXiv. https://arxiv.org/abs/2310.09877.","code":""},{"path":[]},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create and return ALE data, statistics, and plots — ale","text":"Chitu Okoli Chitu.Okoli@skema.edu","code":""},{"path":"https://tripartio.github.io/ale/reference/ale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create and return ALE data, statistics, and plots — ale","text":"","code":"diamonds #> # A tibble: 53,940 × 10 #>    carat cut       color clarity depth table price     x     y     z #>    <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> #>  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 #>  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 #>  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 #>  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 #>  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 #>  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48 #>  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47 #>  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53 #>  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49 #> 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39 #> # ℹ 53,930 more rows set.seed(0) diamonds_sample <- diamonds[sample(nrow(diamonds), 1000), ]  # Split the dataset into training and test sets # https://stackoverflow.com/a/54892459/2449926 set.seed(0) train_test_split <- sample(   c(TRUE, FALSE), nrow(diamonds_sample), replace = TRUE, prob = c(0.8, 0.2) ) diamonds_train <- diamonds_sample[train_test_split, ] diamonds_test <- diamonds_sample[!train_test_split, ]   # Create a GAM model with flexible curves to predict diamond price # Smooth all numeric variables and include all other variables # Build model on training data, not on the full dataset. gam_diamonds <- mgcv::gam(   price ~ s(carat) + s(depth) + s(table) + s(x) + s(y) + s(z) +     cut + color + clarity,   data = diamonds_train ) summary(gam_diamonds) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> price ~ s(carat) + s(depth) + s(table) + s(x) + s(y) + s(z) +  #>     cut + color + clarity #>  #> Parametric coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  3311.6596    82.3642  40.208  < 2e-16 *** #> cut.L         226.8516   174.6866   1.299  0.19447     #> cut.Q         154.4894   136.7646   1.130  0.25900     #> cut.C         -72.0299   114.5065  -0.629  0.52951     #> cut^4          63.4426    91.1593   0.696  0.48667     #> color.L     -1748.6107   119.8822 -14.586  < 2e-16 *** #> color.Q      -499.5061   109.9114  -4.545 6.41e-06 *** #> color.C        -7.7188   102.4915  -0.075  0.93999     #> color^4       132.4433    95.3721   1.389  0.16533     #> color^5      -261.4225    89.7563  -2.913  0.00369 **  #> color^6        34.2734    81.9067   0.418  0.67574     #> clarity.L    4464.0110   271.7651  16.426  < 2e-16 *** #> clarity.Q   -3168.1893   261.5197 -12.115  < 2e-16 *** #> clarity.C    1276.9741   215.1091   5.936 4.44e-09 *** #> clarity^4    -983.6948   161.9254  -6.075 1.97e-09 *** #> clarity^5     557.2408   120.8123   4.612 4.67e-06 *** #> clarity^6    -117.3182   100.4726  -1.168  0.24331     #> clarity^7      -0.9172    87.9078  -0.010  0.99168     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Approximate significance of smooth terms: #>            edf Ref.df     F  p-value     #> s(carat) 8.507  8.820 3.423 0.000389 *** #> s(depth) 1.273  1.510 0.061 0.901571     #> s(table) 5.707  6.842 1.522 0.207715     #> s(x)     5.006  6.125 4.761 8.88e-05 *** #> s(y)     5.644  6.755 9.317  < 2e-16 *** #> s(z)     2.516  3.307 1.128 0.335376     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> R-sq.(adj) =  0.942   Deviance explained = 94.5% #> GCV = 9.5069e+05  Scale est. = 8.9531e+05  n = 801   # Simple ALE without bootstrapping ale_gam_diamonds <- ale(diamonds_test, gam_diamonds) #> Error in ale(diamonds_test, gam_diamonds): object 'diamonds_test' not found   # \\donttest{ # Plot the ALE data gridExtra::grid.arrange(grobs = ale_gam_diamonds$plots, ncol = 2) #> Error in eval(expr, envir, enclos): object 'ale_gam_diamonds' not found  # Bootstrapped ALE # This can be slow, since bootstrapping runs the algorithm boot_it times  # Create ALE with 100 bootstrap samples ale_gam_diamonds_boot <- ale(diamonds_test, gam_diamonds, boot_it = 100) #> Error in ale(diamonds_test, gam_diamonds, boot_it = 100): object 'diamonds_test' not found  # Bootstrapped ALEs print with confidence intervals gridExtra::grid.arrange(grobs = ale_gam_diamonds_boot$plots, ncol = 2) #> Error in eval(expr, envir, enclos): object 'ale_gam_diamonds_boot' not found   # If the predict function you want is non-standard, you may define a # custom predict function. It must return a single numeric vector. custom_predict <- function(object, newdata, type = pred_type) {   predict(object, newdata, type = type, se.fit = TRUE)$fit }  ale_gam_diamonds_custom <- ale(   diamonds_test, gam_diamonds,   pred_fun = custom_predict, pred_type = 'link' ) #> Error in ale(diamonds_test, gam_diamonds, pred_fun = custom_predict, pred_type = \"link\"): object 'diamonds_test' not found  # Plot the ALE data gridExtra::grid.arrange(grobs = ale_gam_diamonds_custom$plots, ncol = 2) #> Error in eval(expr, envir, enclos): object 'ale_gam_diamonds_custom' not found  # }"},{"path":"https://tripartio.github.io/ale/reference/ale_ixn.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and return ALE interaction data, statistics, and plots — ale_ixn","title":"Create and return ALE interaction data, statistics, and plots — ale_ixn","text":"central function manages creation ALE data plots two-way ALE interactions. simple one-way ALE, see ale(). See documentation functionality shared functions. details, see introductory vignette package details examples . plots, n_y_quant number quantiles divide predicted variable (y). middle quantiles grouped specially: middle quantile first confidence interval median_band (median_band[1]) around median. middle quantile special generally represents meaningful interaction. quantiles middle extended borders middle quantile regular borders quantiles. always odd number quantiles: special middle quantile plus equal number quantiles side . n_y_quant even, middle quantile added . n_y_quant odd, number specified used, including middle quantile.","code":""},{"path":"https://tripartio.github.io/ale/reference/ale_ixn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and return ALE interaction data, statistics, and plots — ale_ixn","text":"","code":"ale_ixn(   data,   model,   x1_cols = NULL,   x2_cols = NULL,   y_col = NULL,   ...,   output = c(\"plots\", \"data\"),   pred_fun = function(object, newdata, type = pred_type) {      stats::predict(object =     object, newdata = newdata, type = type)  },   pred_type = \"response\",   x_intervals = 100,   relative_y = \"median\",   y_type = NULL,   median_band = c(0.05, 0.5),   rug_sample_size = 500,   min_rug_per_interval = 1,   ale_xs = NULL,   n_x1_int = 20,   n_x2_int = 20,   n_y_quant = 10,   silent = FALSE )"},{"path":"https://tripartio.github.io/ale/reference/ale_ixn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and return ALE interaction data, statistics, and plots — ale_ixn","text":"data See documentation ale() model See documentation ale() x1_cols, x2_cols character. Vectors column names data two-way interaction ALE data calculated. ALE data calculated x1 column interacting x2 column. x1_cols can standard datatype (logical, factor, numeric) x2_cols can numeric. ixn TRUE, values must provided. y_col See documentation ale() ... used. Inserted require explicit naming subsequent arguments. output See documentation ale() pred_fun, pred_type See documentation ale() x_intervals See documentation ale() relative_y See documentation ale() y_type See documentation ale() median_band See documentation ale() rug_sample_size, min_rug_per_interval See documentation ale() ale_xs See documentation ale() n_x1_int, n_x2_int positive scalar integer. Number intervals x1 x2 axes respectively interaction plot. values ignored x1 x2 numeric (.e, logical factors). n_y_quant positive scalar integer. Number intervals range y values divided colour bands interaction plot. See details. silent See documentation ale()","code":""},{"path":"https://tripartio.github.io/ale/reference/ale_ixn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create and return ALE interaction data, statistics, and plots — ale_ixn","text":"list ALE interaction data tibbles plots. list two levels depth: first level named x1 variables. Within x1 variable list, second level named x2 variables. Within x1-x2 list element, data plot returned requested output argument.","code":""},{"path":"https://tripartio.github.io/ale/reference/ale_ixn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create and return ALE interaction data, statistics, and plots — ale_ixn","text":"","code":"diamonds #> # A tibble: 53,940 × 10 #>    carat cut       color clarity depth table price     x     y     z #>    <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> #>  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 #>  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 #>  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 #>  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 #>  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 #>  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48 #>  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47 #>  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53 #>  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49 #> 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39 #> # ℹ 53,930 more rows set.seed(0) diamonds_sample <- diamonds[sample(nrow(diamonds), 1000), ]  # Split the dataset into training and test sets # https://stackoverflow.com/a/54892459/2449926 set.seed(0) train_test_split <- sample(   c(TRUE, FALSE), nrow(diamonds_sample), replace = TRUE, prob = c(0.8, 0.2) ) diamonds_train <- diamonds_sample[train_test_split, ] diamonds_test <- diamonds_sample[!train_test_split, ]   # Create a GAM model with flexible curves to predict diamond price # Smooth all numeric variables and include all other variables # Build model on training data, not on the full dataset. gam_diamonds <- mgcv::gam(   price ~ s(carat) + s(depth) + s(table) + s(x) + s(y) + s(z) +     cut + color + clarity,   data = diamonds_train ) summary(gam_diamonds) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> price ~ s(carat) + s(depth) + s(table) + s(x) + s(y) + s(z) +  #>     cut + color + clarity #>  #> Parametric coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  3311.6596    82.3642  40.208  < 2e-16 *** #> cut.L         226.8516   174.6866   1.299  0.19447     #> cut.Q         154.4894   136.7646   1.130  0.25900     #> cut.C         -72.0299   114.5065  -0.629  0.52951     #> cut^4          63.4426    91.1593   0.696  0.48667     #> color.L     -1748.6107   119.8822 -14.586  < 2e-16 *** #> color.Q      -499.5061   109.9114  -4.545 6.41e-06 *** #> color.C        -7.7188   102.4915  -0.075  0.93999     #> color^4       132.4433    95.3721   1.389  0.16533     #> color^5      -261.4225    89.7563  -2.913  0.00369 **  #> color^6        34.2734    81.9067   0.418  0.67574     #> clarity.L    4464.0110   271.7651  16.426  < 2e-16 *** #> clarity.Q   -3168.1893   261.5197 -12.115  < 2e-16 *** #> clarity.C    1276.9741   215.1091   5.936 4.44e-09 *** #> clarity^4    -983.6948   161.9254  -6.075 1.97e-09 *** #> clarity^5     557.2408   120.8123   4.612 4.67e-06 *** #> clarity^6    -117.3182   100.4726  -1.168  0.24331     #> clarity^7      -0.9172    87.9078  -0.010  0.99168     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Approximate significance of smooth terms: #>            edf Ref.df     F  p-value     #> s(carat) 8.507  8.820 3.423 0.000389 *** #> s(depth) 1.273  1.510 0.061 0.901571     #> s(table) 5.707  6.842 1.522 0.207715     #> s(x)     5.006  6.125 4.761 8.88e-05 *** #> s(y)     5.644  6.755 9.317  < 2e-16 *** #> s(z)     2.516  3.307 1.128 0.335376     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> R-sq.(adj) =  0.942   Deviance explained = 94.5% #> GCV = 9.5069e+05  Scale est. = 8.9531e+05  n = 801  # ALE two-way interactions ale_ixn_gam_diamonds <- ale_ixn(diamonds_test, gam_diamonds) #> Error in ale_ixn(diamonds_test, gam_diamonds): object 'diamonds_test' not found   # \\donttest{ # Print interaction plots ale_ixn_gam_diamonds$plots |>   purrr::walk(\\(.x1) {  # extract list of x1 ALE outputs     gridExtra::grid.arrange(grobs = .x1, ncol = 2)  # plot all x1 plots   }) #> Error in eval(expr, envir, enclos): object 'ale_ixn_gam_diamonds' not found # }"},{"path":"https://tripartio.github.io/ale/reference/census.html","id":null,"dir":"Reference","previous_headings":"","what":"Census Income — census","title":"Census Income — census","text":"Census data indicates, among details, respondent's income exceeds $50,000 per year. Also known \"Adult\" dataset.","code":""},{"path":"https://tripartio.github.io/ale/reference/census.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Census Income — census","text":"","code":"census"},{"path":"https://tripartio.github.io/ale/reference/census.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Census Income — census","text":"tibble 32,561 rows 15 columns: higher_income TRUE income > $50,000 age continuous workclass Private, Self-emp--inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked fnlwgt continuous. \"proxy demographic background people: 'People similar demographic characteristics similar weights'\" details, see https://www.openml.org/search?type=data&id=1590. education Bachelors, -college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool education_num continuous marital_status Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse occupation Tech-support, Craft-repair, -service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces relationship Wife, -child, Husband, --family, -relative, Unmarried race White, Asian-Pac-Islander, Amer-Indian-Eskimo, , Black sex Female, Male capital_gain continuous capital_loss continuous hours_per_week continuous native_country United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinidad&Tobago, Peru, Hong, Holland-Netherlands dataset licensed Creative Commons Attribution 4.0 International (CC 4.0) license.","code":""},{"path":"https://tripartio.github.io/ale/reference/census.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Census Income — census","text":"Becker,Barry Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20.","code":""},{"path":"https://tripartio.github.io/ale/reference/create_p_funs.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a p-value functions object that can be used to generate p-values — create_p_funs","title":"Create a p-value functions object that can be used to generate p-values — create_p_funs","text":"Calculating p-values trivial ALE statistics ALE non-parametric model-agnostic. ALE non-parametric (, assume particular distribution data), ale package generates p-values calculating ALE many random variables; makes procedure somewhat slow. reason, calculated default; must explicitly requested. ale package model-agnostic (, works kind R model), ale() function always automatically manipulate model object create p-values. can models follow standard R statistical modelling conventions, includes almost built-R algorithms (like stats::lm() stats::glm()) many widely used statistics packages (like mgcv survival), excludes machine learning algorithms (like tidymodels caret). non-standard algorithms, user needs little work help ale function correctly manipulate model object. Approach calculating p-values ale package takes literal frequentist approach calculation p-values. , literally retrains model 1000 times, time modifying adding distinct random variable model. (number iterations customizable rand_it argument.) ALEs ALE statistics calculated random variable. percentiles distribution random-variable ALEs used determine p-values non-random variables. Thus, p-values interpreted frequency random variable ALE statistics exceed value ALE statistic actual variable question. specific steps follows: residuals original model trained training data calculated (residuals actual y target value minus predicted values). closest distribution residuals detected univariateML::model_select(). 1000 new models trained generating random variable time univariateML::rml() training new model random variable added. ALEs ALE statistics calculated random variable. ALE statistic, empirical cumulative distribution function (stats::ecdf()) used create function determine p-values according distribution random variables' ALE statistics. Datasets ale package takes literal frequentist approach calculation p-values, precision p-values depends correctly representing modelling workflow. Specifically, models trained training subset ALE statistics calculated test subset. Thus, random variables trained training subset ALE statistics calculated test subset. Thus model takes training_data test_data distinct arguments. , whatever reason, ALE calculated dataset used train model, dataset entered training_data test_data.","code":""},{"path":"https://tripartio.github.io/ale/reference/create_p_funs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a p-value functions object that can be used to generate p-values — create_p_funs","text":"","code":"create_p_funs(   training_data,   test_data,   model,   random_model_call_string = NULL,   y_col = NULL,   pred_fun = function(object, newdata, type = pred_type) {      stats::predict(object =     object, newdata = newdata, type = type)  },   pred_type = \"response\",   rand_it = 1000,   silent = FALSE,   .testing_mode = FALSE )"},{"path":"https://tripartio.github.io/ale/reference/create_p_funs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a p-value functions object that can be used to generate p-values — create_p_funs","text":"training_data dataframe. dataset originally used train model. See details also documentation ale(). test_data dataframe. dataset used create ALE data. See details. model model object. model used train original training_data. ALE calculated. See details also documentation ale(). random_model_call_string character string. NULL, create_p_funs() tries automatically detect construct call p-values. , function fail early. case, character string full call model must provided includes random variable. See details. y_col See documentation ale() pred_fun, pred_type See documentation ale(). rand_it non-negative integer length 1. Number times model retrained new random variable. default 1000 give reasonably stable p-values. can reduced low 100 faster test runs. silent See See documentation ale() .testing_mode logical. Internal use .","code":""},{"path":"https://tripartio.github.io/ale/reference/create_p_funs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a p-value functions object that can be used to generate p-values — create_p_funs","text":"return value list class c('p_funs', 'ale', 'list') ale_version attribute whose value version ale package used create object. See examples illustration inspect list. elements : value_to_p: list functions named available ALE statistic. function signature function(x) x numeric. function returns p-value (minimum 0; maximum 1) respective statistic based random variable analysis. input x returns p, interpretation p% random variables obtained higher statistic value. example, get p-value NALED 4.2, enter p_funs$value_to_p(4.2). return value 0.03 means 3% random variables obtained NALED greater equal 4.2. p_to_random_value: list functions named available ALE statistic. inverse functions value_to_p. signature function(p) p numeric 0 1. function returns numeric value random variable statistic yield provided p-value. input p returns x, interpretation p% random variables obtained higher statistic value. example, get random variable ALED 0.05 p-value, enter p_funs$p_to_random_value(0.05). return value 102 means 5% random variables obtained ALED greater equal 102. rand_stats: tibble whose rows rand_it iterations random variable analysis whose columns ALE statistics obtained random variable. residuals: actual y_col values training_data minus predicted values model (without random variables) training_data. residual_distribution: closest estimated distribution residuals determined univariateML::rml(). distribution used generate random variables.","code":""},{"path":"https://tripartio.github.io/ale/reference/create_p_funs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a p-value functions object that can be used to generate p-values — create_p_funs","text":"Okoli, Chitu. 2023. “Statistical Inference Using Machine Learning Classical Techniques Based Accumulated Local Effects (ALE).” arXiv. https://arxiv.org/abs/2310.09877.","code":""},{"path":"https://tripartio.github.io/ale/reference/create_p_funs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a p-value functions object that can be used to generate p-values — create_p_funs","text":"","code":"# \\donttest{ # Sample 1000 rows from the diamonds dataset (for a simple example)diamonds set.seed(0) diamonds_sample <- diamonds[sample(nrow(diamonds), 1000), ]  # Split the dataset into training and test sets # https://stackoverflow.com/a/54892459/2449926 set.seed(0) train_test_split <- sample(   c(TRUE, FALSE), nrow(diamonds_sample), replace = TRUE, prob = c(0.8, 0.2) ) diamonds_train <- diamonds_sample[train_test_split, ] diamonds_test <- diamonds_sample[!train_test_split, ]   # Create a GAM model with flexible curves to predict diamond price # Smooth all numeric variables and include all other variables # Build model on training data, not on the full dataset. gam_diamonds <- mgcv::gam(   price ~ s(carat) + s(depth) + s(table) + s(x) + s(y) + s(z) +     cut + color + clarity,   data = diamonds_train ) summary(gam_diamonds) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> price ~ s(carat) + s(depth) + s(table) + s(x) + s(y) + s(z) +  #>     cut + color + clarity #>  #> Parametric coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  3311.6596    82.3642  40.208  < 2e-16 *** #> cut.L         226.8516   174.6866   1.299  0.19447     #> cut.Q         154.4894   136.7646   1.130  0.25900     #> cut.C         -72.0299   114.5065  -0.629  0.52951     #> cut^4          63.4426    91.1593   0.696  0.48667     #> color.L     -1748.6107   119.8822 -14.586  < 2e-16 *** #> color.Q      -499.5061   109.9114  -4.545 6.41e-06 *** #> color.C        -7.7188   102.4915  -0.075  0.93999     #> color^4       132.4433    95.3721   1.389  0.16533     #> color^5      -261.4225    89.7563  -2.913  0.00369 **  #> color^6        34.2734    81.9067   0.418  0.67574     #> clarity.L    4464.0110   271.7651  16.426  < 2e-16 *** #> clarity.Q   -3168.1893   261.5197 -12.115  < 2e-16 *** #> clarity.C    1276.9741   215.1091   5.936 4.44e-09 *** #> clarity^4    -983.6948   161.9254  -6.075 1.97e-09 *** #> clarity^5     557.2408   120.8123   4.612 4.67e-06 *** #> clarity^6    -117.3182   100.4726  -1.168  0.24331     #> clarity^7      -0.9172    87.9078  -0.010  0.99168     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Approximate significance of smooth terms: #>            edf Ref.df     F  p-value     #> s(carat) 8.507  8.820 3.423 0.000389 *** #> s(depth) 1.273  1.510 0.061 0.901571     #> s(table) 5.707  6.842 1.522 0.207715     #> s(x)     5.006  6.125 4.761 8.88e-05 *** #> s(y)     5.644  6.755 9.317  < 2e-16 *** #> s(z)     2.516  3.307 1.128 0.335376     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> R-sq.(adj) =  0.942   Deviance explained = 94.5% #> GCV = 9.5069e+05  Scale est. = 8.9531e+05  n = 801  # Create p-value functions pf_diamonds <- create_p_funs(   diamonds_train,   diamonds_test,   gam_diamonds,   # only 100 iterations for a quick demo; but usually should remain at 1000   rand_it = 100 ) #> Retraining 100 models with random variables... ■                               … #> Retraining 100 models with random variables... ■■                              … #> Retraining 100 models with random variables... ■■■■■■■                         … #> Retraining 100 models with random variables... ■■■■■■■■■■■■■                   … #> Retraining 100 models with random variables... ■■■■■■■■■■■■■■■■■■■■            … #> Retraining 100 models with random variables... ■■■■■■■■■■■■■■■■■■■■■■■■■       …  # Examine the structure of the returned object str(pf_diamonds) #> List of 5 #>  $ value_to_p           :List of 6 #>   ..$ aled     :function (x)   #>   ..$ aler_min :function (x)   #>   ..$ aler_max :function (x)   #>   ..$ naled    :function (x)   #>   ..$ naler_min:function (x)   #>   ..$ naler_max:function (x)   #>  $ p_to_random_value    :List of 6 #>   ..$ aled     :function (p)   #>   ..$ aler_min :function (p)   #>   ..$ aler_max :function (p)   #>   ..$ naled    :function (p)   #>   ..$ naler_min:function (p)   #>   ..$ naler_max:function (p)   #>  $ rand_stats           : tibble [100 × 6] (S3: tbl_df/tbl/data.frame) #>   ..$ aled     : num [1:100] 2.86 0.317 23.377 2.035 24.302 ... #>   ..$ aler_min : num [1:100] -38.5 -1.4 -232.9 -10.4 -110 ... #>   ..$ aler_max : num [1:100] 11.28 3.04 80.86 8.58 133.98 ... #>   ..$ naled    : num [1:100] 0.0299 0 0.8743 0.0149 0.6724 ... #>   ..$ naler_min: num [1:100] 0 0 -3.5 0 -2 -2 0 0 -2 -2 ... #>   ..$ naler_max: num [1:100] 1.49 0 1.49 1.49 1.98 ... #>  $ residuals            : num [1:801(1d)] -1180.2 -1096.3 -216.6 -25.6 -1511.5 ... #>  $ residual_distribution: 'univariateML' Named num [1:4] 9.71 1020.87 2.99 1.24 #>   ..- attr(*, \"names\")= chr [1:4] \"mean\" \"sd\" \"nu\" \"xi\" #>   ..- attr(*, \"model\")= chr \"Skew Student-t\" #>   ..- attr(*, \"density\")= chr \"fGarch::dsstd\" #>   ..- attr(*, \"logLik\")= num -6503 #>   ..- attr(*, \"support\")= num [1:2] -Inf Inf #>   ..- attr(*, \"n\")= int 801 #>   ..- attr(*, \"call\")= language f(x = x, na.rm = na.rm) #>  - attr(*, \"class\")= chr [1:3] \"p_funs\" \"ale\" \"list\" #>  - attr(*, \"ale_version\")=Classes 'package_version', 'numeric_version'  hidden list of 1 #>   ..$ : int [1:3] 0 2 20240109 # In RStudio: View(pf_diamonds)  # Calculate ALEs with p-values ale_gam_diamonds <- ale(   diamonds_test,   gam_diamonds,   p_values = pf_diamonds ) #> Error in ale(diamonds_test, gam_diamonds, p_values = pf_diamonds): object 'diamonds_test' not found  # Plot the ALE data. The horizontal bands in the plots use the p-values. gridExtra::grid.arrange(grobs = ale_gam_diamonds$plots, ncol = 2) #> Error in eval(expr, envir, enclos): object 'ale_gam_diamonds' not found # }"},{"path":"https://tripartio.github.io/ale/reference/model_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"model_bootstrap.R — model_bootstrap","title":"model_bootstrap.R — model_bootstrap","text":"Execute full model bootstrapping ALE calculation bootstrap run","code":""},{"path":"https://tripartio.github.io/ale/reference/model_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"model_bootstrap.R — model_bootstrap","text":"","code":"model_bootstrap(   data,   model = NULL,   ...,   model_call_string = NULL,   boot_it = 100,   seed = 0,   boot_alpha = 0.05,   boot_centre = \"mean\",   output = c(\"ale\", \"model_stats\", \"model_coefs\"),   ale_options = list(),   tidy_options = list(),   glance_options = list(),   silent = FALSE )"},{"path":"https://tripartio.github.io/ale/reference/model_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"model_bootstrap.R — model_bootstrap","text":"data dataframe. Dataset bootstrapped. model See documentation ale() ... used. Inserted require explicit naming subsequent arguments. model_call_string character string. NULL, model_bootstrap() tries automatically detect construct call bootstrapped datasets. , function fail early. case, character string full call model must provided includes boot_data data argument call. See examples. boot_it integer 0 Inf. Number bootstrap iterations. boot_it = 0, model run normal full data bootstrapping. seed integer. Random seed. Supply runs assure identical bootstrap samples generated time data. boot_alpha numeric. confidence level bootstrap confidence intervals 1 - boot_alpha. example, default 0.05 give 95% confidence interval, , 2.5% 97.5% percentile. boot_centre See See documentation ale() output character vector. types bootstraps calculate return: 'ale': Calculate return bootstrapped ALE data plot. 'model_stats': Calculate return bootstrapped overall model statistics. 'model_coefs': Calculate return bootstrapped model coefficients. 'boot_data': Return full data bootstrap iterations. data always calculated needed bootstrap averages. default, returned except included output argument. ale_options, tidy_options, glance_options list named arguments. Arguments pass ale(), broom::tidy(), broom::glance() functions, respectively, beyond (overriding) defaults. particular, obtain p-values ALE statistics, see details. silent See documentation ale()","code":""},{"path":"https://tripartio.github.io/ale/reference/model_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"model_bootstrap.R — model_bootstrap","text":"list tibbles following elements (depending values requested output argument: model_stats: bootstrapped results broom::glance() model_coefs: bootstrapped results broom::tidy() ale: bootstrapped ALE results data: ALE data (see ale() details format) stats: ALE statistics. data duplicated different views might variously useful. column by_term: statistic, estimate, conf.low, median, mean, conf.high. (\"term\" means variable name.) column names compatible broom package. confidence intervals based ale() function defaults; can changed ale_options argument. estimate median mean, depending boot_centre argument. by_statistic: term, estimate, conf.low, median, mean, conf.high. estimate: term, one column per statistic Provided default estimate. view present confidence intervals. plots: ALE plots (see ale() details format) boot_data: full bootstrap data (returned default) values: boot_it, seed, boot_alpha, boot_centre arguments originally passed returned reference.","code":""},{"path":"https://tripartio.github.io/ale/reference/model_bootstrap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"model_bootstrap.R — model_bootstrap","text":"modelling results, without ALE, considered reliable without bootstrapped. large datasets clear separation training testing samples, ale() bootstraps ALE results test data. However, dataset small subdivided training test sets, entire model bootstrapped. , multiple models trained, one bootstrap sample. reliable results average results bootstrap models, however many . details, see vignette small datasets details examples . model_bootstrap() automatically carries full-model bootstrapping suitable small datasets. Specifically, : Creates multiple bootstrap samples (default 100; user can specify number); Creates model bootstrap sample; Calculates model overall statistics, variable coefficients, ALE values model bootstrap sample; Calculates mean, median, lower upper confidence intervals values across bootstrap samples. P-values broom::tidy() summary statistics provide p-values normal, situation somewhat complicated p-values ALE statistics. challenge procedure obtaining p-values slow: involves retraining model 1000 times. Thus, efficient calculate p-values every execution model_bootstrap(). Although ale() function provides 'auto' option creating p-values, option disabled model_bootstrap() far slow: involve retraining model 1000 times number bootstrap iterations. Rather, must first create p-values function object using procedure described help(create_p_funs). name p-values object p_funs, can request p-values time run model_bootstrap() passing argument ale_options = list(p_values = p_funs).","code":""},{"path":"https://tripartio.github.io/ale/reference/model_bootstrap.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"model_bootstrap.R — model_bootstrap","text":"Okoli, Chitu. 2023. “Statistical Inference Using Machine Learning Classical Techniques Based Accumulated Local Effects (ALE).” arXiv. https://arxiv.org/abs/2310.09877.","code":""},{"path":"https://tripartio.github.io/ale/reference/model_bootstrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"model_bootstrap.R — model_bootstrap","text":"","code":"# attitude dataset attitude #>    rating complaints privileges learning raises critical advance #> 1      43         51         30       39     61       92      45 #> 2      63         64         51       54     63       73      47 #> 3      71         70         68       69     76       86      48 #> 4      61         63         45       47     54       84      35 #> 5      81         78         56       66     71       83      47 #> 6      43         55         49       44     54       49      34 #> 7      58         67         42       56     66       68      35 #> 8      71         75         50       55     70       66      41 #> 9      72         82         72       67     71       83      31 #> 10     67         61         45       47     62       80      41 #> 11     64         53         53       58     58       67      34 #> 12     67         60         47       39     59       74      41 #> 13     69         62         57       42     55       63      25 #> 14     68         83         83       45     59       77      35 #> 15     77         77         54       72     79       77      46 #> 16     81         90         50       72     60       54      36 #> 17     74         85         64       69     79       79      63 #> 18     65         60         65       75     55       80      60 #> 19     65         70         46       57     75       85      46 #> 20     50         58         68       54     64       78      52 #> 21     50         40         33       34     43       64      33 #> 22     64         61         52       62     66       80      41 #> 23     53         66         52       50     63       80      37 #> 24     40         37         42       58     50       57      49 #> 25     63         54         42       48     66       75      33 #> 26     66         77         66       63     88       76      72 #> 27     78         75         58       74     80       78      49 #> 28     48         57         44       45     51       83      38 #> 29     85         85         71       71     77       74      55 #> 30     82         82         39       59     64       78      39  ## ALE for general additive models (GAM) ## GAM is tweaked to work on the small dataset. gam_attitude <- mgcv::gam(rating ~ complaints + privileges + s(learning) +                             raises + s(critical) + advance,                           data = attitude) summary(gam_attitude) #>  #> Family: gaussian  #> Link function: identity  #>  #> Formula: #> rating ~ complaints + privileges + s(learning) + raises + s(critical) +  #>     advance #>  #> Parametric coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept) 36.97245   11.60967   3.185 0.004501 **  #> complaints   0.60933    0.13297   4.582 0.000165 *** #> privileges  -0.12662    0.11432  -1.108 0.280715     #> raises       0.06222    0.18900   0.329 0.745314     #> advance     -0.23790    0.14807  -1.607 0.123198     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Approximate significance of smooth terms: #>               edf Ref.df     F p-value   #> s(learning) 1.923  2.369 3.761  0.0312 * #> s(critical) 2.296  2.862 3.272  0.0565 . #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> R-sq.(adj) =  0.776   Deviance explained = 83.9% #> GCV = 47.947  Scale est. = 33.213    n = 30  # Full model bootstrapping # Only 3 bootstrap iterations for a rapid example; default is 100 # Increase value of boot_it for more realistic results mb_gam <- model_bootstrap(   attitude,   gam_attitude,   boot_it = 3 ) #> Creating and analyzing models ■                                  0% |  ETA: ?  # \\donttest{ # If the model is not standard, supply model_call_string with # 'data = boot_data' in the string (not as a direct argument to [model_bootstrap()]) mb_gam <- model_bootstrap(   attitude,   model_call_string = 'mgcv::gam(     rating ~ complaints + privileges + s(learning) +       raises + s(critical) + advance,     data = boot_data   )',   boot_it = 3 )  # Model statistics and coefficients mb_gam$model_stats #> # A tibble: 3 × 7 #>   name        estimate conf.low  mean median conf.high    sd #>   <chr>          <dbl>    <dbl> <dbl>  <dbl>     <dbl> <dbl> #> 1 df              17.3     15.1  17.3   18.0      18.9  2.08 #> 2 df.residual     12.7     11.1  12.7   12.0      14.9  2.08 #> 3 nobs            30       30    30     30        30    0    mb_gam$model_coefs #> # A tibble: 2 × 7 #>   term        estimate conf.low  mean median conf.high std.error #>   <chr>          <dbl>    <dbl> <dbl>  <dbl>     <dbl>     <dbl> #> 1 s(learning)     8.21     7.39  8.21   8.36      8.91     0.813 #> 2 s(critical)     4.12     1.29  4.12   5.64      5.66     2.65   # Plot ALE gridExtra::grid.arrange(grobs = mb_gam$ale$plots, ncol = 2)  # }"},{"path":"https://tripartio.github.io/ale/reference/var_cars.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi-variable transformation of the mtcars dataset. — var_cars","title":"Multi-variable transformation of the mtcars dataset. — var_cars","text":"transformation mtcars dataset R produce small dataset fundamental datatypes: logical, factor, ordered, integer, double. transformations obvious, two noteworthy: unordered factor, country car manufacturer obtained based row names mtcars. var_cars version row names. ordered factor, gears 3, 4, 5 encoded 'three', 'four', 'five', respectively. text labels make explicit variable ordinal, yet number names make order crystal clear. original description mtcars dataset: data extracted 1974 Motor Trend US magazine, comprises fuel consumption 10 aspects automobile design performance 32 automobiles (1973--74 models).","code":""},{"path":"https://tripartio.github.io/ale/reference/var_cars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi-variable transformation of the mtcars dataset. — var_cars","text":"","code":"var_cars"},{"path":"https://tripartio.github.io/ale/reference/var_cars.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Multi-variable transformation of the mtcars dataset. — var_cars","text":"tibble 32 observations 12 variables. mpg double: Miles/(US) gallon cyl integer: Number cylinders disp double: Displacement (cu..) hp double: Gross horsepower drat double: Rear axle ratio wt double: Weight (1000 lbs) qsec double: 1/4 mile time vs logical: Engine (0 = V-shaped, 1 = straight) logical: Transmission (0 = automatic, 1 = manual) gear ordered: Number forward gears carb integer: Number carburetors country factor: Country car manufacturer","code":""},{"path":"https://tripartio.github.io/ale/reference/var_cars.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Multi-variable transformation of the mtcars dataset. — var_cars","text":"Henderson Velleman (1981) comment footnote Table 1: 'Hocking (original transcriber)'s noncrucial coding Mazda's rotary engine straight six-cylinder engine Porsche's flat engine V engine, well inclusion diesel Mercedes 240D, retained enable direct comparisons made previous analyses.'","code":""},{"path":"https://tripartio.github.io/ale/reference/var_cars.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multi-variable transformation of the mtcars dataset. — var_cars","text":"Henderson Velleman (1981), Building multiple regression models interactively. Biometrics, 37, 391--411.","code":""}]
